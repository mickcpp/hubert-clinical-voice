{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e6RbMAq5maju"
      },
      "source": [
        "# Setup generico ML/DL\n",
        "Template per esperimenti di deep learning - Tesi"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zou8Xj0Y1zP7"
      },
      "outputs": [],
      "source": [
        "# ============================================================\n",
        "# EXECUTION FLAGS - Control what to run\n",
        "# ============================================================\n",
        "\n",
        "RUN_DATASET_ANALYSIS = False\n",
        "RUN_TRAINING_CV = True\n",
        "RUN_XAI = False"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wsS8lFqrmuks"
      },
      "source": [
        "## 1. Mount Google Drive"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "inkFsDEYM6pA"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "import os\n",
        "\n",
        "if not os.path.ismount('/content/drive'):\n",
        "    drive.mount('/content/drive')\n",
        "else:\n",
        "    print(\"✓ Drive already connected\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s7c3rxjImyxe"
      },
      "source": [
        "## 2. Import Libraries\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2jr9juicbuNp"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Libreria per grafici\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Framework deep learning\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "# Librerie per manipolazione dati\n",
        "import numpy as np          # calcoli matematici su array/matrici\n",
        "import pandas as pd         # manipolazione dati (tabelle, CSV)\n",
        "\n",
        "import os\n",
        "import librosa\n",
        "\n",
        "from transformers import HubertModel, Wav2Vec2FeatureExtractor\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "from sklearn.metrics import accuracy_score, balanced_accuracy_score, f1_score, confusion_matrix, roc_auc_score\n",
        "import matplotlib.pyplot as plt\n",
        "from tqdm.auto import tqdm\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "import time\n",
        "\n",
        "# AMP (Automatic Mixed Precision)\n",
        "from torch.cuda.amp import autocast, GradScaler\n",
        "# Scheduler per learning rate adaptation\n",
        "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
        "\n",
        "print(\"✓ Librerie importate con successo\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SGXZvV-Vm2CO"
      },
      "source": [
        "## 3. Verify GPU\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zjuypQzncwA8"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Verifica GPU\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(f\"✓ Device: {device}\")\n",
        "if torch.cuda.is_available():\n",
        "    print(f\"  GPU: {torch.cuda.get_device_name(0)}\")\n",
        "else:\n",
        "    print(\"ATTENZIONE: GPU non disponibile!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KUC1Aa5DnH8l"
      },
      "source": [
        "## 4. Configure Reproducibility"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZcLCQV4dfGl5"
      },
      "outputs": [],
      "source": [
        "SEED = 42\n",
        "\n",
        "np.random.seed(SEED)              # NumPy (CPU)\n",
        "torch.manual_seed(SEED)           # PyTorch CPU\n",
        "\n",
        "# PyTorch GPU\n",
        "if torch.cuda.is_available():\n",
        "    torch.cuda.manual_seed(SEED)\n",
        "    torch.cuda.manual_seed_all(SEED)\n",
        "\n",
        "print(\"✓ Seed impostati per riproducibilità\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S4NoPX18nkNo"
      },
      "source": [
        "## 5. Set Display Options"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YPLMFj4chz4r"
      },
      "outputs": [],
      "source": [
        "# Pandas display options\n",
        "pd.set_option('display.max_rows', 100)\n",
        "pd.set_option('display.max_columns', 50)\n",
        "pd.set_option('display.width', 1000)\n",
        "pd.set_option('display.precision', 4)  # Decimali per float (opzionale)\n",
        "\n",
        "# Matplotlib inline\n",
        "%matplotlib inline\n",
        "\n",
        "# Dimensione default plot più grande e leggibile\n",
        "import matplotlib\n",
        "matplotlib.rcParams['figure.figsize'] = (10, 6)\n",
        "matplotlib.rcParams['font.size'] = 10  # Font leggibile (opzionale)\n",
        "\n",
        "print(\"✓ Opzioni display configurate\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z_fkqvImnaXb"
      },
      "source": [
        "## 6. Define and Verify Project Structure\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "o5kajeR3lzvI"
      },
      "outputs": [],
      "source": [
        "# Base path del progetto\n",
        "BASE_PATH = '/content/drive/MyDrive/Tesi/'\n",
        "\n",
        "# Percorsi specifici\n",
        "DATA_PATH = BASE_PATH + 'data/'\n",
        "DATA_RAW_PATH = DATA_PATH + 'raw/'\n",
        "DATA_PROCESSED_PATH = DATA_PATH + 'processed/'\n",
        "MODELS_PATH = BASE_PATH + 'models/'\n",
        "RESULTS_PATH = BASE_PATH + 'results/'\n",
        "\n",
        "def setup_experiment(experiment_name):\n",
        "    \"\"\"\n",
        "    Crea struttura cartelle per esperimento specifico.\n",
        "    Chiamare all'inizio di ogni notebook esperimento.\n",
        "\n",
        "    Args:\n",
        "        experiment_name: nome univoco (es. 'cnn_baseline', 'lstm_v2')\n",
        "\n",
        "    Returns:\n",
        "        dict con tutti i path per l'esperimento\n",
        "    \"\"\"\n",
        "    # Sottocartelle per questo esperimento\n",
        "    exp_models_path = os.path.join(MODELS_PATH, experiment_name)\n",
        "    exp_results_path = os.path.join(RESULTS_PATH, experiment_name)\n",
        "\n",
        "    # Crea se non esistono\n",
        "    os.makedirs(exp_models_path, exist_ok=True)\n",
        "    os.makedirs(exp_results_path, exist_ok=True)\n",
        "\n",
        "    paths = {\n",
        "        'models': exp_models_path,\n",
        "        'results': exp_results_path,\n",
        "        'data_raw': DATA_RAW_PATH,\n",
        "        'data_processed': DATA_PROCESSED_PATH\n",
        "    }\n",
        "\n",
        "    print(f\"✓ Esperimento '{experiment_name}' configurato\")\n",
        "    print(f\"  Models: {exp_models_path}\")\n",
        "    print(f\"  Results: {exp_results_path}\")\n",
        "\n",
        "    return paths\n",
        "\n",
        "# Verifica struttura base (come prima)\n",
        "print(\"Verifica struttura base:\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "base_paths = [\n",
        "    ('Base', BASE_PATH),\n",
        "    ('Data', DATA_PATH),\n",
        "    ('Data Raw', DATA_RAW_PATH),\n",
        "    ('Data Processed', DATA_PROCESSED_PATH),\n",
        "    ('Models', MODELS_PATH),\n",
        "    ('Results', RESULTS_PATH)\n",
        "]\n",
        "\n",
        "all_ok = True\n",
        "for path_name, path in base_paths:\n",
        "    if os.path.exists(path):\n",
        "        print(f\"✓ {path_name:<20} {path}\")\n",
        "    else:\n",
        "        print(f\"✗ {path_name:<20} NOT FOUND: {path}\")\n",
        "        all_ok = False\n",
        "\n",
        "print(\"=\" * 60)\n",
        "if all_ok:\n",
        "    print(\"✓ Struttura base completa!\")\n",
        "else:\n",
        "    print(\"ATTENZIONE: crea le cartelle mancanti in Drive\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FOO-QPgvUW6S"
      },
      "source": [
        "## Setup experiment"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "o0vU3wqPUZDV"
      },
      "outputs": [],
      "source": [
        "# Nome esperimento\n",
        "EXPERIMENT_NAME = 'hubert_attention_pooling'\n",
        "\n",
        "# Setup paths per esperimento\n",
        "paths = setup_experiment(EXPERIMENT_NAME)\n",
        "\n",
        "print(f\"✓ Path configurati per '{EXPERIMENT_NAME}'\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eDwE4HIrq_Na"
      },
      "outputs": [],
      "source": [
        "# ============================================================\n",
        "# INSTALLAZIONE DIPENDENZE\n",
        "# ============================================================\n",
        "\n",
        "!pip install -q transformers==4.47.1\n",
        "!pip install -q librosa==0.10.2\n",
        "!pip install -q audiomentations==0.35.0\n",
        "\n",
        "print(\"✓ Librerie installate!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SVqDH6YCrjzz"
      },
      "outputs": [],
      "source": [
        "# Parametri globali\n",
        "CONFIG = {\n",
        "    # Dati\n",
        "    'csv_path': DATA_RAW_PATH + 'dataset_free_speech.csv',\n",
        "    'audio_dir': DATA_RAW_PATH + 'audio/',\n",
        "\n",
        "    # Preprocessing\n",
        "    'target_sr': 16000,  # HuBERT richiede 16kHz - (scritto per chiarezza, ma il codice usa processor.sampling_rate)\n",
        "    'max_duration': 30,  # Max 30 secondi (tronca se più lunghi)\n",
        "\n",
        "    # Modello\n",
        "    'model_name': 'facebook/hubert-base-ls960',\n",
        "    'freeze_layers': 9,  # Freeze layer 0-8, train 9-11\n",
        "    'pooling_type': 'attention',  # 'attention' o 'mean'\n",
        "\n",
        "    # Training\n",
        "    'n_folds': 5,  # 5-fold cross-validation\n",
        "    'batch_size': 4,\n",
        "    'num_epochs': 30,\n",
        "    'learning_rate': 5e-5,  # Standard per fine-tuning\n",
        "    'weight_decay': 5e-4,  # Regularization\n",
        "\n",
        "    'scheduler': {\n",
        "        'factor': 0.5,       # Riduce LR a 50% quando val_loss non migliora\n",
        "        'patience': 3,       # Aspetta 3 epoch prima di ridurre\n",
        "        'min_lr': 1e-6,\n",
        "    },\n",
        "\n",
        "    # Classificatore\n",
        "    'hidden_dim': 256,\n",
        "    'dropout': 0.25,\n",
        "    'num_classes': 2,\n",
        "\n",
        "    'early_stopping': {\n",
        "        'warmup_epochs': 8,\n",
        "        'max_loss_threshold': 0.75,\n",
        "        'use_composite_score': True,\n",
        "        'composite_weights': {\n",
        "            'auc': 0.45,        # 45% discriminazione\n",
        "            'balacc': 0.35,     # 35% balance\n",
        "            'loss': -0.2,      # -20% penalizza loss\n",
        "        },\n",
        "        'patience': 10,\n",
        "    },\n",
        "\n",
        "    # Augmentation\n",
        "    'use_augmentation': True,\n",
        "    'aug_prob': 0.165,\n",
        "\n",
        "    # Riproducibilità\n",
        "    'seed': 42\n",
        "}\n",
        "\n",
        "print(\"✓ Configurazione caricata:\")\n",
        "for key, value in CONFIG.items():\n",
        "    print(f\"  {key}: {value}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_BznUHhtayy6"
      },
      "outputs": [],
      "source": [
        "from transformers import Wav2Vec2FeatureExtractor\n",
        "\n",
        "processor = Wav2Vec2FeatureExtractor.from_pretrained(CONFIG['model_name'])\n",
        "\n",
        "print(f\"✓ Processor caricato (sampling_rate: {processor.sampling_rate} Hz)\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2NaE9sCfrwa3"
      },
      "outputs": [],
      "source": [
        "# ============================================================\n",
        "# CARICAMENTO DATASET\n",
        "# ============================================================\n",
        "\n",
        "# Carica CSV\n",
        "df = pd.read_csv(CONFIG['csv_path'], sep=';')\n",
        "\n",
        "# Crea label binaria (0=Controllo, 1=Paziente)\n",
        "df['label'] = (df['Tipo soggetto'] == 'Paziente').astype(int)\n",
        "\n",
        "# ============================================================\n",
        "# ESCLUSIONE FILE PROBLEMATICI (basato su analisi qualitativa)\n",
        "# ============================================================\n",
        "problematic_files = [\n",
        "    'D_AP_F_51_2024_10_23_Italian.wav'\n",
        "]\n",
        "\n",
        "print(f\"Dataset prima dell'esclusione: {len(df)} campioni\")\n",
        "df = df[~df['FileName'].isin(problematic_files)]\n",
        "df = df.reset_index(drop=True)\n",
        "print(f\"Dataset dopo esclusione: {len(df)} campioni ({len(problematic_files)} esclusi)\")\n",
        "for fname in problematic_files:\n",
        "    print(f\"  - Escluso: {fname}\")\n",
        "\n",
        "# ============================================================\n",
        "# CARICAMENTO ONSET MAP (per trimming file-specific)\n",
        "# ============================================================\n",
        "onset_stats_path = os.path.join(DATA_PATH, 'dataset_free_speech_analysis', 'audio_statistics.csv')\n",
        "\n",
        "if os.path.exists(onset_stats_path):\n",
        "    stats_df = pd.read_csv(onset_stats_path)\n",
        "\n",
        "    # Applica margine conservativo (taglia solo 50% del rilevato)\n",
        "    TRIM_REDUCTION_FACTOR = 0.5\n",
        "    stats_df['trim_amount_adjusted'] = stats_df['trim_amount'] * TRIM_REDUCTION_FACTOR\n",
        "\n",
        "    # Crea dizionario: filename -> trim_amount (in secondi)\n",
        "    onset_map = dict(zip(stats_df['filename'], stats_df['trim_amount_adjusted']))\n",
        "\n",
        "    print(f\"\\n✓ Onset map caricato (CONSERVATIVO): {len(onset_map)} file\")\n",
        "    print(f\"  Trim originale: {stats_df['trim_amount'].mean():.2f}s ± {stats_df['trim_amount'].std():.2f}s\")\n",
        "    print(f\"  Trim adjusted:  {stats_df['trim_amount_adjusted'].mean():.2f}s ± {stats_df['trim_amount_adjusted'].std():.2f}s\")\n",
        "else:\n",
        "    print(f\"\\n⚠ WARNING: Onset map non trovato\")\n",
        "    onset_map = {}\n",
        "\n",
        "# Verifica esistenza file audio\n",
        "missing_files = []\n",
        "\n",
        "for idx, row in df.iterrows():\n",
        "    audio_path = os.path.join(CONFIG['audio_dir'], row['FileName'])\n",
        "    if not os.path.exists(audio_path):\n",
        "        missing_files.append(row['FileName'])\n",
        "\n",
        "if missing_files:\n",
        "    print(f\"ATTENZIONE: {len(missing_files)} file audio non trovati!\")\n",
        "    print(\"Primi 5:\", missing_files[:5])\n",
        "else:\n",
        "    print(\"✓ Tutti i file audio trovati!\")\n",
        "\n",
        "# Dataset info\n",
        "print(f\"\\n=== DATASET INFO ===\")\n",
        "print(f\"Totale campioni: {len(df)}\")\n",
        "print(f\"Controlli (0): {(df['label']==0).sum()}\")\n",
        "print(f\"Pazienti (1): {(df['label']==1).sum()}\")\n",
        "print(f\"Ratio Paziente/Controllo: {(df['label']==1).sum()/(df['label']==0).sum():.2f}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_FwZcQ8Ir0Cq"
      },
      "outputs": [],
      "source": [
        "class SpeechDataset(Dataset):\n",
        "    def __init__(self, dataframe, audiodir, processor, max_duration=30, augment=False, onset_map=None):\n",
        "        self.df = dataframe.reset_index(drop=True)\n",
        "        self.audiodir = audiodir\n",
        "        self.processor = processor\n",
        "        self.target_sr = processor.sampling_rate\n",
        "        self.max_duration = max_duration\n",
        "        self.max_samples = self.target_sr * max_duration\n",
        "        self.augment = augment\n",
        "        self.onset_map = onset_map if onset_map is not None else {}\n",
        "\n",
        "        if self.augment:\n",
        "            from audiomentations import Compose, AddGaussianNoise, Gain\n",
        "\n",
        "            aug_prob = CONFIG['aug_prob']\n",
        "\n",
        "            self.augmentor = Compose([\n",
        "                # 1. Additive Noise (simula ambiente registrazione)\n",
        "                AddGaussianNoise(\n",
        "                    min_amplitude=0.002, max_amplitude=0.015, p=aug_prob),\n",
        "\n",
        "                # 2. Volume/Gain (simula distanza microfono)\n",
        "                Gain(min_gain_in_db=-6, max_gain_in_db=6, p=aug_prob*0.85),\n",
        "            ])\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.df)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        row = self.df.iloc[idx]\n",
        "        audiopath = os.path.join(self.audiodir, row['FileName'])\n",
        "        label = row['label']\n",
        "        filename = row['FileName']\n",
        "\n",
        "        # Carica audio a sample rate corretto\n",
        "        waveform, sr_original = librosa.load(audiopath, sr=self.target_sr, mono=True)\n",
        "\n",
        "        # ============================================================\n",
        "        # STEP 1: ONSET TRIMMING (file-specific, rimuove silenzio iniziale)\n",
        "        # ============================================================\n",
        "        if filename in self.onset_map:\n",
        "            trim_amount_sec = self.onset_map[filename]\n",
        "            trim_samples = int(trim_amount_sec * self.target_sr)\n",
        "\n",
        "            # Trim solo se ha senso (evita trim > lunghezza audio)\n",
        "            if 0 < trim_samples < len(waveform):\n",
        "                waveform = waveform[trim_samples:]\n",
        "\n",
        "        # ============================================================\n",
        "        # STEP 2: AUGMENTATION (se richiesta, su audio già trimmato)\n",
        "        # ============================================================\n",
        "        if self.augment:\n",
        "            waveform = self.augmentor(samples=waveform, sample_rate=self.target_sr)\n",
        "\n",
        "        # ============================================================\n",
        "        # STEP 3: TRUNCATION a max_duration (DOPO trimming)\n",
        "        # ============================================================\n",
        "        if len(waveform) > self.max_samples:\n",
        "            waveform = waveform[:self.max_samples]\n",
        "\n",
        "        # Waveform RAW numpy → HF processor viene chiamato in collate_fn\n",
        "        return {\n",
        "            'input_values': waveform,\n",
        "            'labels': label,\n",
        "        }\n",
        "\n",
        "print(\"✓ SpeechDataset definito\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_V68sdqIjbNn"
      },
      "outputs": [],
      "source": [
        "def collate_fn(batch):\n",
        "    input_values = [item['input_values'] for item in batch]\n",
        "    labels = [item['labels'] for item in batch]\n",
        "\n",
        "    batch_encoded = processor(\n",
        "        input_values,\n",
        "        sampling_rate=processor.sampling_rate,\n",
        "        return_tensors=\"pt\",\n",
        "        padding=True,\n",
        "        return_attention_mask=True\n",
        "    )\n",
        "\n",
        "    batch_encoded['labels'] = torch.tensor(labels, dtype=torch.long)\n",
        "\n",
        "    return batch_encoded\n",
        "\n",
        "print(\"✓ Collate function definita\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lfVOWoKKr7xr"
      },
      "outputs": [],
      "source": [
        "# ============================================================\n",
        "# MODELLO: HuBERT + ATTENTION POOLING + MLP CLASSIFIER\n",
        "#\n",
        "# Fonti:\n",
        "# - HuBERT: Hsu et al. 2021 \"HuBERT: Self-Supervised Speech Representation Learning\"\n",
        "# - Attention Pooling: Okabe et al. 2018 \"Attentive Statistics Pooling for Deep Speaker Recognition\"\n",
        "# ============================================================\n",
        "\n",
        "class AttentionPooling(nn.Module):\n",
        "    \"\"\"\n",
        "    Attention-based pooling per aggregare sequenza embeddings.\n",
        "\n",
        "    Input: (batch, time, features)\n",
        "    Output: (batch, features), attention_weights (batch, time, 1)\n",
        "    \"\"\"\n",
        "    def __init__(self, input_dim):\n",
        "        super().__init__()\n",
        "        self.attention = nn.Linear(input_dim, 1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # x: (batch, time, input_dim)\n",
        "        attn_scores = self.attention(x)  # (batch, time, 1)\n",
        "        attn_weights = F.softmax(attn_scores, dim=1)  # Normalize over time\n",
        "        pooled = torch.sum(attn_weights * x, dim=1)  # (batch, input_dim)\n",
        "        return pooled, attn_weights\n",
        "\n",
        "\n",
        "class HuBERTClassifier(nn.Module):\n",
        "    \"\"\"\n",
        "    HuBERT encoder + Attention Pooling + MLP classifier.\n",
        "\n",
        "    Architecture:\n",
        "    - HuBERT-Base (12 layer, 768-d embeddings)\n",
        "    - Layer 0-8: FROZEN (general acoustic knowledge)\n",
        "    - Layer 9-11: FINE-TUNED (task-specific adaptation)\n",
        "    - Attention Pooling (learnable aggregation)\n",
        "    - MLP: 768 → 256 → 2 (binary classification)\n",
        "    \"\"\"\n",
        "    def __init__(self, model_name, freeze_layers=9, hidden_dim=256,\n",
        "                 dropout=0.3, num_classes=2, pooling_type='attention'):\n",
        "        super().__init__()\n",
        "\n",
        "        # HuBERT encoder\n",
        "        self.hubert = HubertModel.from_pretrained(model_name)\n",
        "        self.hidden_size = self.hubert.config.hidden_size  # 768 for base\n",
        "\n",
        "        # Freeze primi N layer\n",
        "        for layer_idx in range(freeze_layers):\n",
        "            for param in self.hubert.encoder.layers[layer_idx].parameters():\n",
        "                param.requires_grad = False\n",
        "\n",
        "        # Pooling layer\n",
        "        self.pooling_type = pooling_type\n",
        "        if pooling_type == 'attention':\n",
        "            self.pooling = AttentionPooling(self.hidden_size)\n",
        "        # else: use mean pooling (implemented in forward)\n",
        "\n",
        "        # Classifier MLP\n",
        "        self.classifier = nn.Sequential(\n",
        "            nn.Dropout(dropout),\n",
        "            nn.Linear(self.hidden_size, hidden_dim),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(dropout),\n",
        "            nn.Linear(hidden_dim, num_classes)\n",
        "        )\n",
        "\n",
        "    def forward(self, input_values, attention_mask=None, return_attention=False):\n",
        "        # HuBERT encoding\n",
        "        outputs = self.hubert(input_values, attention_mask=attention_mask)\n",
        "        embeddings = outputs.last_hidden_state  # (batch, time, 768)\n",
        "\n",
        "        # Pooling\n",
        "        if self.pooling_type == 'attention':\n",
        "            pooled, attn_weights = self.pooling(embeddings)\n",
        "        else:\n",
        "            pooled = embeddings.mean(dim=1)\n",
        "            attn_weights = None\n",
        "\n",
        "        logits = self.classifier(pooled)\n",
        "\n",
        "        if return_attention and attn_weights is not None:\n",
        "            return logits, attn_weights\n",
        "        return logits\n",
        "\n",
        "print(\"✓ Modello HuBERTClassifier definito\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yVqcslssr9zj"
      },
      "outputs": [],
      "source": [
        "# ============================================================\n",
        "# TRAINING FUNCTIONS\n",
        "# ============================================================\n",
        "\n",
        "def train_epoch(model, dataloader, criterion, optimizer, device, scaler):\n",
        "    \"\"\"\n",
        "    Training epoch con Automatic Mixed Precision (AMP).\n",
        "\n",
        "    AMP strategy:\n",
        "    - Forward pass in mixed precision (fp16/fp32 automatico)\n",
        "    - Loss computation in fp16\n",
        "    - Gradient scaling per evitare underflow\n",
        "    - Metrics computation in fp32\n",
        "\n",
        "    Args:\n",
        "        scaler: torch.cuda.amp.GradScaler per gradient scaling\n",
        "    \"\"\"\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "    all_preds = []\n",
        "    all_labels = []\n",
        "\n",
        "    for batch in tqdm(dataloader, desc=\"Training\", leave=False):\n",
        "        input_values = batch['input_values'].to(device)\n",
        "        attention_mask = batch['attention_mask'].to(device)\n",
        "        labels = batch['labels'].to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # Forward pass in mixed precision\n",
        "        with autocast():\n",
        "            logits = model(input_values, attention_mask=attention_mask)\n",
        "            loss = criterion(logits, labels)\n",
        "\n",
        "        # Backward pass con gradient scaling\n",
        "        scaler.scale(loss).backward()\n",
        "        scaler.step(optimizer)\n",
        "        scaler.update()\n",
        "\n",
        "        # Metrics: in float32\n",
        "        # .item() converte loss a Python float (fp32)\n",
        "        total_loss += loss.item()\n",
        "\n",
        "        # .argmax() e .cpu() mantengono precisione corretta\n",
        "        preds = logits.argmax(dim=1).cpu().numpy()\n",
        "        all_preds.extend(preds)\n",
        "        all_labels.extend(labels.cpu().numpy())\n",
        "\n",
        "    # Calcola metriche (in fp32)\n",
        "    avg_loss = total_loss / len(dataloader)\n",
        "    accuracy = accuracy_score(all_labels, all_preds)\n",
        "    bal_accuracy = balanced_accuracy_score(all_labels, all_preds)\n",
        "\n",
        "    return avg_loss, accuracy, bal_accuracy\n",
        "\n",
        "print(\"✓ train_epoch definita\")\n",
        "\n",
        "def validate_epoch(model, dataloader, criterion, device):\n",
        "    \"\"\"\n",
        "    Validation epoch con AMP.\n",
        "\n",
        "    IMPORTANTE: Anche in validation usiamo autocast per consistency\n",
        "    con training. NO gradient computation, ma stessa precisione forward.\n",
        "    \"\"\"\n",
        "    model.eval()\n",
        "    total_loss = 0\n",
        "    all_preds = []\n",
        "    all_labels = []\n",
        "    all_probs = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for batch in tqdm(dataloader, desc=\"Validation\", leave=False):\n",
        "            input_values = batch['input_values'].to(device)\n",
        "            attention_mask = batch['attention_mask'].to(device)\n",
        "            labels = batch['labels'].to(device)\n",
        "\n",
        "            # Forward in mixed precision (consistency con training)\n",
        "            with autocast():\n",
        "                logits = model(input_values, attention_mask=attention_mask)\n",
        "                loss = criterion(logits, labels)\n",
        "\n",
        "            # Probabilità per AUC: convert to float32 esplicitamente\n",
        "            # softmax su logits fp16 → output fp16 → .float() → fp32\n",
        "            probs = torch.softmax(logits.float(), dim=1)[:, 1].cpu().numpy()\n",
        "\n",
        "            total_loss += loss.item()\n",
        "            preds = logits.argmax(dim=1).cpu().numpy()\n",
        "            all_preds.extend(preds)\n",
        "            all_labels.extend(labels.cpu().numpy())\n",
        "            all_probs.extend(probs)\n",
        "\n",
        "    # Metriche in fp32\n",
        "    avg_loss = total_loss / len(dataloader)\n",
        "    accuracy = accuracy_score(all_labels, all_preds)\n",
        "    bal_accuracy = balanced_accuracy_score(all_labels, all_preds)\n",
        "    f1 = f1_score(all_labels, all_preds, average='weighted')\n",
        "    auc = roc_auc_score(all_labels, all_probs)\n",
        "    cm = confusion_matrix(all_labels, all_preds)\n",
        "\n",
        "    return avg_loss, accuracy, bal_accuracy, f1, auc, cm, all_preds, all_labels, all_probs\n",
        "\n",
        "print(\"✓ validate_epoch definita\")\n",
        "\n",
        "\n",
        "print(\"✓ Training functions definite\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7xZWToEkSMWK"
      },
      "source": [
        "## K-Fold Cross Validation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "orbb_uqpsAd7"
      },
      "outputs": [],
      "source": [
        "# ============================================================\n",
        "# MAIN TRAINING LOOP - 5-FOLD CROSS-VALIDATION\n",
        "# ============================================================\n",
        "\n",
        "# Setup\n",
        "torch.manual_seed(CONFIG['seed'])\n",
        "np.random.seed(CONFIG['seed'])\n",
        "\n",
        "if RUN_TRAINING_CV:\n",
        "    # Stratified K-Fold\n",
        "    skf = StratifiedKFold(n_splits=CONFIG['n_folds'], shuffle=True, random_state=CONFIG['seed'])\n",
        "\n",
        "    # Storage risultati\n",
        "    fold_results = []\n",
        "    all_fold_histories = []\n",
        "\n",
        "    print(f\"\\n{'='*60}\")\n",
        "    print(f\"INIZIO TRAINING - {CONFIG['n_folds']}-FOLD CROSS-VALIDATION\")\n",
        "    print(f\"{'='*60}\\n\")\n",
        "\n",
        "    for fold, (train_idx, val_idx) in enumerate(skf.split(df, df['label']), 1):\n",
        "        print(f\"\\n{'='*60}\")\n",
        "        print(f\"FOLD {fold}/{CONFIG['n_folds']}\")\n",
        "        print(f\"{'='*60}\")\n",
        "\n",
        "        # Split train/val\n",
        "        train_df = df.iloc[train_idx]\n",
        "        val_df = df.iloc[val_idx]\n",
        "\n",
        "        print(f\"Train: {len(train_df)} samples (Pz:{(train_df['label']==1).sum()}, Ctrl:{(train_df['label']==0).sum()})\")\n",
        "        print(f\"Val:   {len(val_df)} samples (Pz:{(val_df['label']==1).sum()}, Ctrl:{(val_df['label']==0).sum()})\")\n",
        "\n",
        "        # Datasets\n",
        "        train_dataset = SpeechDataset(\n",
        "            train_df,\n",
        "            CONFIG['audio_dir'],\n",
        "            processor=processor,\n",
        "            max_duration=CONFIG['max_duration'],\n",
        "            augment=CONFIG['use_augmentation'],\n",
        "            onset_map=onset_map\n",
        "        )\n",
        "\n",
        "        val_dataset = SpeechDataset(\n",
        "            val_df,\n",
        "            CONFIG['audio_dir'],\n",
        "            processor=processor,\n",
        "            max_duration=CONFIG['max_duration'],\n",
        "            augment=False, # No augmentation in validation\n",
        "            onset_map=onset_map\n",
        "        )\n",
        "\n",
        "        # DataLoaders\n",
        "        train_loader = DataLoader(\n",
        "            train_dataset,\n",
        "            batch_size=CONFIG['batch_size'],\n",
        "            shuffle=True,\n",
        "            num_workers=0,\n",
        "            collate_fn=collate_fn\n",
        "        )\n",
        "\n",
        "        val_loader = DataLoader(\n",
        "            val_dataset,\n",
        "            batch_size=CONFIG['batch_size'],\n",
        "            shuffle=False,\n",
        "            num_workers=0,\n",
        "            collate_fn=collate_fn\n",
        "        )\n",
        "\n",
        "        # Modello\n",
        "        model = HuBERTClassifier(\n",
        "            model_name=CONFIG['model_name'],\n",
        "            freeze_layers=CONFIG['freeze_layers'],\n",
        "            hidden_dim=CONFIG['hidden_dim'],\n",
        "            dropout=CONFIG['dropout'],\n",
        "            num_classes=CONFIG['num_classes'],\n",
        "            pooling_type=CONFIG['pooling_type']\n",
        "        ).to(device)\n",
        "\n",
        "        # Loss e optimizer CON CLASS WEIGHTS\n",
        "        # Calcola pesi per bilanciare sbilanciamento\n",
        "        n_ctrl = (train_df['label'] == 0).sum()\n",
        "        n_pz = (train_df['label'] == 1).sum()\n",
        "        total = len(train_df)\n",
        "\n",
        "        weight_ctrl = total / (2 * n_ctrl)\n",
        "        weight_pz = total / (2 * n_pz)\n",
        "\n",
        "        class_weights = torch.tensor(\n",
        "            [weight_ctrl, weight_pz],\n",
        "            dtype=torch.float32\n",
        "        ).to(device)\n",
        "\n",
        "        print(f\"  Class weights: Ctrl={weight_ctrl:.3f}, Pz={weight_pz:.3f}\")\n",
        "\n",
        "        criterion = nn.CrossEntropyLoss(weight=class_weights)\n",
        "        optimizer = torch.optim.AdamW(model.parameters(), lr=CONFIG['learning_rate'], weight_decay=CONFIG['weight_decay'])\n",
        "\n",
        "        # AMP Scaler (per questo fold)\n",
        "        scaler = GradScaler()\n",
        "\n",
        "        scheduler = ReduceLROnPlateau(\n",
        "            optimizer,\n",
        "            mode='min',\n",
        "            factor=CONFIG['scheduler']['factor'],\n",
        "            patience=CONFIG['scheduler']['patience'],\n",
        "            min_lr=CONFIG['scheduler']['min_lr']\n",
        "        )\n",
        "\n",
        "        # Training loop\n",
        "        best_val_loss = float('inf')\n",
        "        best_val_auc = 0.0\n",
        "        best_val_bal_acc = 0.0\n",
        "        best_composite_score = -float('inf')\n",
        "        patience_counter = 0\n",
        "\n",
        "        history = {\n",
        "            'train_loss': [],\n",
        "            'train_bal_acc': [],\n",
        "            'val_loss': [],\n",
        "            'val_acc': [],\n",
        "            'val_bal_acc': [],\n",
        "            'val_auc': []\n",
        "        }\n",
        "\n",
        "        for epoch in range(1, CONFIG['num_epochs'] + 1):\n",
        "            print(f\"\\nEpoch {epoch}/{CONFIG['num_epochs']}\")\n",
        "\n",
        "            # Train con scaler\n",
        "            train_loss, train_acc, train_bal_acc = train_epoch(\n",
        "                model, train_loader, criterion, optimizer, device, scaler\n",
        "            )\n",
        "\n",
        "            # Validate (no scaler needed, solo autocast)\n",
        "            val_loss, val_acc, val_bal_acc, val_f1, val_auc, val_cm, _, _, _ = validate_epoch(\n",
        "                model, val_loader, criterion, device\n",
        "            )\n",
        "\n",
        "            # Log\n",
        "            history['train_loss'].append(train_loss)\n",
        "            history['train_bal_acc'].append(train_bal_acc)\n",
        "            history['val_loss'].append(val_loss)\n",
        "            history['val_acc'].append(val_acc)\n",
        "            history['val_bal_acc'].append(val_bal_acc)\n",
        "            history['val_auc'].append(val_auc)\n",
        "\n",
        "            print(f\" Train Loss: {train_loss:.4f} | Acc: {train_acc:.4f} | Bal_Acc: {train_bal_acc:.4f}\")\n",
        "            print(f\" Val Loss: {val_loss:.4f} | Acc: {val_acc:.4f} | Bal_Acc: {val_bal_acc:.4f} | F1: {val_f1:.4f} | AUC: {val_auc:.4f}\")\n",
        "            print(f\" LR: {optimizer.param_groups[0]['lr']:.2e}\")\n",
        "\n",
        "            # Stampa componenti confusion matrix\n",
        "            tn, fp, fn, tp = val_cm.ravel()\n",
        "            print(f\"   CM: [[TN={tn}, FP={fp}], [FN={fn}, TP={tp}]]\")\n",
        "\n",
        "            # STEP SCHEDULER (RIDUCE LR SE VAL_LOSS NON MIGLIORA)\n",
        "            scheduler.step(val_loss)\n",
        "            current_lr = optimizer.param_groups[0]['lr']\n",
        "            print(f\"Current LR: {current_lr:.2e}\")\n",
        "\n",
        "            # ===== EARLY STOPPING CON COMPOSITE SCORE =====\n",
        "            WARMUP_EPOCHS = CONFIG['early_stopping']['warmup_epochs']\n",
        "            MAX_LOSS_THRESHOLD = CONFIG['early_stopping']['max_loss_threshold']\n",
        "            USE_COMPOSITE = CONFIG['early_stopping']['use_composite_score']\n",
        "\n",
        "            # Calcola composite score (se attivato)\n",
        "            if USE_COMPOSITE:\n",
        "                weights = CONFIG['early_stopping']['composite_weights']\n",
        "                composite_score = (\n",
        "                    weights['auc'] * val_auc +\n",
        "                    weights['balacc'] * val_bal_acc +\n",
        "                    weights['loss'] * val_loss  # Negativo nel config (-0.2)\n",
        "                )\n",
        "                print(f\"Composite Score: {composite_score:.4f} (AUC:{val_auc:.4f} BalAcc:{val_bal_acc:.4f} Loss:{val_loss:.4f})\")\n",
        "            else:\n",
        "                composite_score = val_auc  # Fallback\n",
        "\n",
        "            # FASE 1: WARMUP (primi N epoch)\n",
        "            # Obiettivo: Trova un buon modello iniziale con loss-based semplice\n",
        "            if epoch <= WARMUP_EPOCHS:\n",
        "                if val_loss < MAX_LOSS_THRESHOLD and val_auc > best_val_auc:\n",
        "                    best_val_auc = val_auc\n",
        "                    best_val_loss = val_loss\n",
        "                    best_val_bal_acc = val_bal_acc\n",
        "                    best_composite_score = composite_score\n",
        "                    patience_counter = 0\n",
        "\n",
        "                    # Salva checkpoint\n",
        "                    checkpoint = {\n",
        "                        'model_state_dict': model.state_dict(),\n",
        "                        'scaler_state_dict': scaler.state_dict(),\n",
        "                        'epoch': epoch,\n",
        "                        'val_loss': val_loss,\n",
        "                        'val_auc': float(val_auc),\n",
        "                        'val_bal_acc': float(val_bal_acc),\n",
        "                    }\n",
        "\n",
        "                    model_path = os.path.join(paths['models'], f'best_model_fold{fold}.pth')\n",
        "                    torch.save(checkpoint, model_path)\n",
        "                    print(f\" [WARMUP] Best model saved! AUC: {val_auc:.4f}, BalAcc: {val_bal_acc:.4f}, Loss: {val_loss:.4f}\")\n",
        "\n",
        "                    # Verifica file\n",
        "                    max_wait = 10\n",
        "                    waited = 0\n",
        "                    while waited < max_wait:\n",
        "                        if os.path.exists(model_path):\n",
        "                            file_size = os.path.getsize(model_path)\n",
        "                            if file_size > 1024:\n",
        "                                print(f\"File verificato: {file_size / 1e6:.1f} MB\")\n",
        "                                break\n",
        "                        time.sleep(1)\n",
        "                        waited += 1\n",
        "\n",
        "                    if waited >= max_wait:\n",
        "                        print(f\"  WARNING: File non trovato dopo {max_wait}s!\")\n",
        "                else:\n",
        "                    patience_counter += 1\n",
        "                    if val_loss >= MAX_LOSS_THRESHOLD:\n",
        "                        print(f\"  [WARMUP] Loss {val_loss:.4f} > {MAX_LOSS_THRESHOLD}\")\n",
        "\n",
        "            # FASE 2: POST-WARMUP (dopo epoch N)\n",
        "            # Obiettivo: Usa composite score per bilanciare AUC + BalAcc + Loss\n",
        "            else:\n",
        "                if USE_COMPOSITE:\n",
        "                    # POST-WARMUP CON COMPOSITE SCORE\n",
        "                    if val_loss < MAX_LOSS_THRESHOLD:\n",
        "                        # Salva se composite score migliora\n",
        "                        if composite_score > best_composite_score:\n",
        "                            best_composite_score = composite_score\n",
        "                            best_val_auc = val_auc\n",
        "                            best_val_loss = val_loss\n",
        "                            best_val_bal_acc = val_bal_acc\n",
        "                            patience_counter = 0\n",
        "\n",
        "                            checkpoint = {\n",
        "                                'model_state_dict': model.state_dict(),\n",
        "                                'scaler_state_dict': scaler.state_dict(),\n",
        "                                'epoch': epoch,\n",
        "                                'val_loss': val_loss,\n",
        "                                'val_auc': float(val_auc),\n",
        "                                'val_bal_acc': float(val_bal_acc),\n",
        "                                'composite_score': float(composite_score),\n",
        "                            }\n",
        "\n",
        "                            model_path = os.path.join(paths['models'], f'best_model_fold{fold}.pth')\n",
        "                            torch.save(checkpoint, model_path)\n",
        "                            print(f\" [COMPOSITE] Best model saved! Score: {composite_score:.4f}\")\n",
        "                            print(f\"   AUC: {val_auc:.4f}, BalAcc: {val_bal_acc:.4f}, Loss: {val_loss:.4f}\")\n",
        "\n",
        "                            # Verifica file\n",
        "                            max_wait = 10\n",
        "                            waited = 0\n",
        "                            while waited < max_wait:\n",
        "                                if os.path.exists(model_path):\n",
        "                                    file_size = os.path.getsize(model_path)\n",
        "                                    if file_size > 1024:\n",
        "                                        print(f\"File verificato: {file_size / 1e6:.1f} MB\")\n",
        "                                        break\n",
        "                                time.sleep(1)\n",
        "                                waited += 1\n",
        "\n",
        "                            if waited >= max_wait:\n",
        "                                print(f\"  WARNING: File non trovato dopo {max_wait}s!\")\n",
        "                        else:\n",
        "                            patience_counter += 1\n",
        "                            print(f\"  Composite score {composite_score:.4f} ≤ best {best_composite_score:.4f}\")\n",
        "                    else:\n",
        "                        # Val loss esplosa oltre threshold\n",
        "                        patience_counter += 1\n",
        "                        print(f\"  Loss {val_loss:.4f} > {MAX_LOSS_THRESHOLD} (explosion)\")\n",
        "\n",
        "                else:\n",
        "                    # FALLBACK: Se composite disattivato, usa loss-based\n",
        "                    if val_loss < MAX_LOSS_THRESHOLD and val_auc > best_val_auc:\n",
        "                        best_val_auc = val_auc\n",
        "                        best_val_loss = val_loss\n",
        "                        patience_counter = 0\n",
        "                        # ... (salva modello come in warmup)\n",
        "\n",
        "            # Check patience finale\n",
        "            if patience_counter >= CONFIG['early_stopping']['patience']:\n",
        "                print(f\"⏹  Early stopping at epoch {epoch} (patience exhausted)\")\n",
        "                break\n",
        "\n",
        "        # Carica best model (solo model weights, scaler non serve per evaluation)\n",
        "        model_path = os.path.join(paths['models'], f'best_model_fold{fold}.pth')\n",
        "\n",
        "        print(f\"  ⏳ Caricamento best model...\")\n",
        "        max_wait = 30\n",
        "        waited = 0\n",
        "\n",
        "        while not os.path.exists(model_path) and waited < max_wait:\n",
        "            time.sleep(1)\n",
        "            waited += 1\n",
        "\n",
        "        if os.path.exists(model_path):\n",
        "            try:\n",
        "                checkpoint = torch.load(model_path)\n",
        "                model.load_state_dict(checkpoint['model_state_dict'])\n",
        "                print(f\"  ✓ Best model restored (epoch {checkpoint['epoch']}, val_loss: {checkpoint['val_loss']:.4f})\")\n",
        "            except Exception as e:\n",
        "                print(f\"  ❌ Errore nel caricare il modello: {e}\")\n",
        "                print(f\"  → Uso il modello corrente\")\n",
        "        else:\n",
        "            print(f\"  ⚠️  Best model non trovato dopo {max_wait}s!\")\n",
        "            print(f\"  → Uso il modello corrente\")\n",
        "\n",
        "        final_val_loss, final_val_acc, final_val_bal_acc, final_val_f1, final_val_auc, final_cm, preds, labels, probs = validate_epoch(\n",
        "            model, val_loader, criterion, device\n",
        "        )\n",
        "\n",
        "        # Estrai componenti confusion matrix\n",
        "        tn, fp, fn, tp = final_cm.ravel()\n",
        "\n",
        "        print(f\"\\n{'='*60}\")\n",
        "        print(f\"FOLD {fold} FINAL RESULTS\")\n",
        "        print(f\"{'='*60}\")\n",
        "        print(f\"Val Accuracy: {final_val_acc:.4f}\")\n",
        "        print(f\"Val Balanced Accuracy: {final_val_bal_acc:.4f}\")\n",
        "        print(f\"Val F1-Score: {final_val_f1:.4f}\")\n",
        "        print(f\"Val AUC-ROC: {final_val_auc:.4f}\")\n",
        "        print(f\"\\nConfusion Matrix:\")\n",
        "        print(final_cm)\n",
        "        print(f\"TN={tn}, FP={fp}, FN={fn}, TP={tp}\")\n",
        "\n",
        "        # Store results (include probs/labels for ROC plot)\n",
        "        fold_results.append({\n",
        "            'fold': fold,\n",
        "            'val_acc': final_val_acc,\n",
        "            'val_bal_acc': final_val_bal_acc,\n",
        "            'val_f1': final_val_f1,\n",
        "            'val_auc': final_val_auc,\n",
        "            'tn': int(tn),\n",
        "            'fp': int(fp),\n",
        "            'fn': int(fn),\n",
        "            'tp': int(tp),\n",
        "            'probs': probs,   # Probabilità per ROC curve\n",
        "            'labels': labels  # Labels vere per ROC curve\n",
        "        })\n",
        "        all_fold_histories.append(history)\n",
        "\n",
        "    print(f\"\\n\\n{'='*60}\")\n",
        "    print(\"CROSS-VALIDATION COMPLETA\")\n",
        "    print(f\"{'='*60}\\n\")\n",
        "else:\n",
        "    print(\"-> Skipping Cross-Validation (RUN_TRAINING_CV = False)\")\n",
        "    print(\"   Set RUN_TRAINING_CV = True to run 5-fold CV\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CIertrZvsEVD"
      },
      "outputs": [],
      "source": [
        "# ============================================================\n",
        "# RISULTATI FINALI CROSS-VALIDATION\n",
        "# ============================================================\n",
        "\n",
        "if RUN_TRAINING_CV:\n",
        "    # Calcola metriche aggregate\n",
        "    val_accs = [r['val_acc'] for r in fold_results]\n",
        "    val_bal_accs = [r['val_bal_acc'] for r in fold_results]\n",
        "    val_f1s = [r['val_f1'] for r in fold_results]\n",
        "    val_aucs = [r['val_auc'] for r in fold_results]\n",
        "\n",
        "    print(\"=== RISULTATI 5-FOLD CROSS-VALIDATION ===\\n\")\n",
        "    print(f\"Accuracy:          {np.mean(val_accs):.4f} ± {np.std(val_accs):.4f}\")\n",
        "    print(f\"Balanced Accuracy: {np.mean(val_bal_accs):.4f} ± {np.std(val_bal_accs):.4f}\")\n",
        "    print(f\"F1-Score:          {np.mean(val_f1s):.4f} ± {np.std(val_f1s):.4f}\")\n",
        "    print(f\"AUC-ROC:           {np.mean(val_aucs):.4f} ± {np.std(val_aucs):.4f}\")\n",
        "\n",
        "    print(\"\\n=== PER-FOLD BREAKDOWN ===\")\n",
        "    for r in fold_results:\n",
        "        print(f\"Fold {r['fold']}: Acc={r['val_acc']:.4f}, Bal_Acc={r['val_bal_acc']:.4f}, F1={r['val_f1']:.4f}, AUC={r['val_auc']:.4f}\")\n",
        "\n",
        "    # Salva risultati\n",
        "    results_df = pd.DataFrame(fold_results)\n",
        "    results_df.to_csv(os.path.join(paths['results'], 'cv_results.csv'), index=False)\n",
        "\n",
        "    print(f\"\\n✓ Risultati salvati in: {paths['results']}/cv_results.csv\")\n",
        "\n",
        "    # ============================================================\n",
        "    # PLOT LEARNING CURVES CON TRAIN/VAL GAP\n",
        "    # ============================================================\n",
        "\n",
        "    fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
        "\n",
        "    # Colori consistenti per fold\n",
        "    colors = plt.cm.tab10(np.linspace(0, 1, len(all_fold_histories)))\n",
        "\n",
        "    # Plot 1: Training Loss (tutte fold)\n",
        "    for fold_idx, history in enumerate(all_fold_histories, 1):\n",
        "        axes[0, 0].plot(history['train_loss'], alpha=0.6, color=colors[fold_idx-1],\n",
        "                        linestyle='-', label=f'Fold {fold_idx}')\n",
        "    axes[0, 0].set_title('Training Loss per Fold', fontsize=12, fontweight='bold')\n",
        "    axes[0, 0].set_xlabel('Epoch')\n",
        "    axes[0, 0].set_ylabel('Loss')\n",
        "    axes[0, 0].legend(loc='best')\n",
        "    axes[0, 0].grid(True, alpha=0.3)\n",
        "\n",
        "    # Plot 2: Validation Loss (tutte fold)\n",
        "    for fold_idx, history in enumerate(all_fold_histories, 1):\n",
        "        axes[0, 1].plot(history['val_loss'], alpha=0.6, color=colors[fold_idx-1],\n",
        "                        linestyle='--', label=f'Fold {fold_idx}')\n",
        "    axes[0, 1].set_title('Validation Loss per Fold', fontsize=12, fontweight='bold')\n",
        "    axes[0, 1].set_xlabel('Epoch')\n",
        "    axes[0, 1].set_ylabel('Loss')\n",
        "    axes[0, 1].legend(loc='best')\n",
        "    axes[0, 1].grid(True, alpha=0.3)\n",
        "\n",
        "    # Plot 3: Validation Balanced Accuracy\n",
        "    for fold_idx, history in enumerate(all_fold_histories, 1):\n",
        "        axes[1, 0].plot(history['val_bal_acc'], alpha=0.6, color=colors[fold_idx-1],\n",
        "                        label=f'Fold {fold_idx}')\n",
        "    axes[1, 0].set_title('Validation Balanced Accuracy per Fold', fontsize=12, fontweight='bold')\n",
        "    axes[1, 0].set_xlabel('Epoch')\n",
        "    axes[1, 0].set_ylabel('Balanced Accuracy')\n",
        "    axes[1, 0].legend(loc='best')\n",
        "    axes[1, 0].grid(True, alpha=0.3)\n",
        "    axes[1, 0].set_ylim([0.4, 1.0])\n",
        "\n",
        "    # Plot 4: Validation AUC-ROC\n",
        "    for fold_idx, history in enumerate(all_fold_histories, 1):\n",
        "        axes[1, 1].plot(history['val_auc'], alpha=0.6, color=colors[fold_idx-1],\n",
        "                        label=f'Fold {fold_idx}')\n",
        "    axes[1, 1].set_title('Validation AUC-ROC per Fold', fontsize=12, fontweight='bold')\n",
        "    axes[1, 1].set_xlabel('Epoch')\n",
        "    axes[1, 1].set_ylabel('AUC-ROC')\n",
        "    axes[1, 1].legend(loc='best')\n",
        "    axes[1, 1].grid(True, alpha=0.3)\n",
        "    axes[1, 1].set_ylim([0.4, 1.0])\n",
        "\n",
        "    plt.tight_layout()\n",
        "\n",
        "    # Salva in multipli formati (PNG + PDF per tesi)\n",
        "    plt.savefig(os.path.join(paths['results'], 'learning_curves.png'), dpi=300, bbox_inches='tight')\n",
        "    plt.savefig(os.path.join(paths['results'], 'learning_curves.pdf'), bbox_inches='tight')\n",
        "    plt.show()\n",
        "\n",
        "    print(\"✓ Plot salvati:\")\n",
        "    print(f\"  - {paths['results']}/learning_curves.png (300 DPI)\")\n",
        "    print(f\"  - {paths['results']}/learning_curves.pdf (vettoriale)\")\n",
        "\n",
        "    # ============================================================\n",
        "    # PLOT FINALE: ROC MEDIA + CONFUSION MATRIX AGGREGATA\n",
        "    # ============================================================\n",
        "\n",
        "    from sklearn.metrics import roc_curve, auc as sklearn_auc\n",
        "\n",
        "    print(\"\\n\" + \"=\"*60)\n",
        "    print(\"GENERAZIONE PLOT FINALE: ROC MEDIA + CM AGGREGATA\")\n",
        "    print(\"=\"*60)\n",
        "\n",
        "    # ------------------------------\n",
        "    # 1. ROC CURVE MEDIA\n",
        "    # ------------------------------\n",
        "\n",
        "    # Griglia comune FPR per interpolazione (standard practice)\n",
        "    mean_fpr = np.linspace(0, 1, 100)\n",
        "\n",
        "    tprs = []  # TPR interpolati per ogni fold\n",
        "    aucs = []  # AUC per ogni fold\n",
        "\n",
        "    fig_final, axes = plt.subplots(1, 2, figsize=(16, 6))\n",
        "\n",
        "    # Plot ROC per ogni fold + interpola\n",
        "    for i, result in enumerate(fold_results, 1):\n",
        "        fpr, tpr, _ = roc_curve(result['labels'], result['probs'])\n",
        "        roc_auc = sklearn_auc(fpr, tpr)\n",
        "        aucs.append(roc_auc)\n",
        "\n",
        "        # Interpola TPR sulla griglia comune FPR\n",
        "        tpr_interp = np.interp(mean_fpr, fpr, tpr)\n",
        "        tpr_interp[0] = 0.0  # Forza (0,0)\n",
        "        tprs.append(tpr_interp)\n",
        "\n",
        "        # Plot fold individuale (trasparente)\n",
        "        axes[0].plot(fpr, tpr, alpha=0.3, linewidth=1)\n",
        "\n",
        "    # Calcola ROC media e std\n",
        "    mean_tpr = np.mean(tprs, axis=0)\n",
        "    mean_tpr[-1] = 1.0  # Forza (1,1)\n",
        "    mean_auc = np.mean(aucs)\n",
        "    std_auc = np.std(aucs)\n",
        "\n",
        "    std_tpr = np.std(tprs, axis=0)\n",
        "    tprs_upper = np.minimum(mean_tpr + std_tpr, 1)\n",
        "    tprs_lower = np.maximum(mean_tpr - std_tpr, 0)\n",
        "\n",
        "    # Plot ROC media (linea spessa)\n",
        "    axes[0].plot(mean_fpr, mean_tpr, color='blue', linewidth=3,\n",
        "                 label=f'Mean ROC (AUC = {mean_auc:.3f} ± {std_auc:.3f})')\n",
        "\n",
        "    # Banda di confidenza (±1 std)\n",
        "    axes[0].fill_between(mean_fpr, tprs_lower, tprs_upper,\n",
        "                         color='grey', alpha=0.3, label='±1 std')\n",
        "\n",
        "    # Linea diagonale (random classifier)\n",
        "    axes[0].plot([0, 1], [0, 1], 'k--', linewidth=1.5, label='Chance (AUC = 0.5)')\n",
        "\n",
        "    axes[0].set_xlim([0.0, 1.0])\n",
        "    axes[0].set_ylim([0.0, 1.05])\n",
        "    axes[0].set_xlabel('False Positive Rate', fontsize=12, fontweight='bold')\n",
        "    axes[0].set_ylabel('True Positive Rate', fontsize=12, fontweight='bold')\n",
        "    axes[0].set_title(f'Mean ROC Curve ({CONFIG[\"n_folds\"]}-Fold CV)',\n",
        "                      fontsize=13, fontweight='bold')\n",
        "    axes[0].legend(loc='lower right', fontsize=9)\n",
        "    axes[0].grid(True, alpha=0.3)\n",
        "    axes[0].set_aspect('equal')\n",
        "\n",
        "    # ------------------------------\n",
        "    # 2. CONFUSION MATRIX AGGREGATA\n",
        "    # ------------------------------\n",
        "\n",
        "    # Somma conteggi di tutti i fold\n",
        "    cm_aggregated = np.array([\n",
        "        [sum(r['tn'] for r in fold_results), sum(r['fp'] for r in fold_results)],\n",
        "        [sum(r['fn'] for r in fold_results), sum(r['tp'] for r in fold_results)]\n",
        "    ])\n",
        "\n",
        "    total_predictions = cm_aggregated.sum()\n",
        "\n",
        "    # Plot heatmap con colori\n",
        "    im = axes[1].imshow(cm_aggregated, interpolation='nearest', cmap='Blues')\n",
        "    axes[1].figure.colorbar(im, ax=axes[1], fraction=0.046, pad=0.04)\n",
        "\n",
        "    # Annotazioni: conteggi + percentuali\n",
        "    thresh = cm_aggregated.max() / 2.\n",
        "    for i in range(2):\n",
        "        for j in range(2):\n",
        "            count = cm_aggregated[i, j]\n",
        "            percentage = count / total_predictions * 100\n",
        "            axes[1].text(j, i, f'{count}\\n({percentage:.1f}%)',\n",
        "                        ha=\"center\", va=\"center\", fontsize=14, fontweight='bold',\n",
        "                        color=\"white\" if cm_aggregated[i, j] > thresh else \"black\")\n",
        "\n",
        "    # Etichette assi\n",
        "    axes[1].set_xticks([0, 1])\n",
        "    axes[1].set_yticks([0, 1])\n",
        "    axes[1].set_xticklabels(['Control', 'Patient'], fontsize=11)\n",
        "    axes[1].set_yticklabels(['Control', 'Patient'], fontsize=11)\n",
        "    axes[1].set_ylabel('True Label', fontsize=12, fontweight='bold')\n",
        "    axes[1].set_xlabel('Predicted Label', fontsize=12, fontweight='bold')\n",
        "    axes[1].set_title(f'Aggregated Confusion Matrix ({CONFIG[\"n_folds\"]}-Fold CV)\\n'\n",
        "                     f'N = {total_predictions} total predictions',\n",
        "                     fontsize=13, fontweight='bold')\n",
        "\n",
        "    plt.tight_layout()\n",
        "\n",
        "    # Salva plot finale\n",
        "    plt.savefig(os.path.join(paths['results'], 'roc_cm_final.png'), dpi=300, bbox_inches='tight')\n",
        "    plt.savefig(os.path.join(paths['results'], 'roc_cm_final.pdf'), bbox_inches='tight')\n",
        "    plt.show()\n",
        "\n",
        "    print(\"✓ Plot finale salvati:\")\n",
        "    print(f\"  - {paths['results']}/roc_cm_final.png (300 DPI)\")\n",
        "    print(f\"  - {paths['results']}/roc_cm_final.pdf (vettoriale)\")\n",
        "    print(f\"\\n  ROC Media: AUC = {mean_auc:.3f} ± {std_auc:.3f}\")\n",
        "    print(f\"    CM Aggregata:\\n{cm_aggregated}\")\n",
        "    print(f\"   TN={cm_aggregated[0,0]}, FP={cm_aggregated[0,1]}, \"\n",
        "          f\"FN={cm_aggregated[1,0]}, TP={cm_aggregated[1,1]}\")\n",
        "else:\n",
        "    print(\"⏭  Skipping CV results (not executed)\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LHxrnUix0sKV"
      },
      "source": [
        "## Analisi statistica dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cNBR3o_D0pcE"
      },
      "outputs": [],
      "source": [
        "# ============================================================\n",
        "# AUDIO DATASET ANALYSIS - Thesis Version\n",
        "# ============================================================\n",
        "# Comprehensive analysis of speech audio dataset:\n",
        "# - Duration statistics (total and effective after onset trimming)\n",
        "# - Speech onset detection using energy-based method\n",
        "# - Audio quality assessment (silence ratio, RMS energy, clipping)\n",
        "# - Problematic file identification\n",
        "# ============================================================\n",
        "\n",
        "import os\n",
        "import librosa\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from tqdm import tqdm\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "\n",
        "# ============================================================\n",
        "# SILENCE RATIO COMPUTATION\n",
        "# ============================================================\n",
        "\n",
        "def compute_silence_ratio(rms, method='adaptive'):\n",
        "    \"\"\"\n",
        "    Compute silence ratio using robust energy threshold method.\n",
        "\n",
        "    The silence ratio quantifies the proportion of low-energy frames\n",
        "    in an audio signal, indicating the amount of silence or background\n",
        "    noise relative to active speech.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    rms : np.ndarray\n",
        "        Root Mean Square (RMS) energy values for each audio frame\n",
        "    method : str, default='adaptive'\n",
        "        Method for silence threshold calculation:\n",
        "        - 'adaptive': Threshold = 10% of peak RMS energy\n",
        "                     Frames below this threshold are classified as silence.\n",
        "                     This method is robust to gain variations across recordings\n",
        "                     and is standard practice in speech signal processing.\n",
        "        - 'absolute': Fixed threshold = 0.01 (requires manual tuning)\n",
        "        - 'percentile': Threshold = 5th percentile of RMS distribution\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    silence_ratio : float\n",
        "        Fraction of frames classified as silence [0, 1]\n",
        "    silence_threshold : float\n",
        "        Energy threshold value used for silence detection\n",
        "\n",
        "    Notes\n",
        "    -----\n",
        "    The 'adaptive' method (10% of peak) is recommended because:\n",
        "    1. Automatically adjusts to recording volume differences\n",
        "    2. Robust to microphone gain variations\n",
        "    3. Standard approach in speech processing literature\n",
        "\n",
        "    Example: If peak RMS = 0.8, threshold = 0.08\n",
        "             Frames with RMS < 0.08 are classified as silence\n",
        "    \"\"\"\n",
        "    if len(rms) == 0 or np.all(np.isnan(rms)):\n",
        "        return 0.0, 0.0\n",
        "\n",
        "    if method == 'adaptive':\n",
        "        rms_peak = np.max(rms)\n",
        "        if rms_peak > 0:\n",
        "            silence_threshold = rms_peak * 0.1\n",
        "        else:\n",
        "            silence_threshold = 0.01\n",
        "\n",
        "    elif method == 'absolute':\n",
        "        silence_threshold = 0.01\n",
        "\n",
        "    elif method == 'percentile':\n",
        "        silence_threshold = np.percentile(rms, 5)\n",
        "\n",
        "    else:\n",
        "        raise ValueError(f\"Unknown method: {method}\")\n",
        "\n",
        "    silence_ratio = np.sum(rms < silence_threshold) / len(rms)\n",
        "\n",
        "    return silence_ratio, silence_threshold\n",
        "\n",
        "\n",
        "# ============================================================\n",
        "# SPEECH ONSET DETECTION\n",
        "# ============================================================\n",
        "\n",
        "def detect_speech_onset(y, sr, energy_threshold_percentile=20,\n",
        "                        min_speech_duration=0.3, pre_speech_margin=0.5):\n",
        "    \"\"\"\n",
        "    Detect speech onset time using energy-based adaptive threshold method.\n",
        "\n",
        "    This function identifies the start of meaningful speech content by\n",
        "    analyzing RMS energy patterns. It distinguishes between low-energy\n",
        "    background noise and high-energy speech segments.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    y : np.ndarray\n",
        "        Audio waveform (time-domain signal)\n",
        "    sr : int\n",
        "        Sample rate in Hz\n",
        "    energy_threshold_percentile : float, default=20\n",
        "        Percentile of RMS distribution used as energy threshold.\n",
        "        Default 20 means: frames below the 20th percentile are classified\n",
        "        as background noise/silence; frames above are potential speech.\n",
        "        Lower values = more permissive (easier to detect speech)\n",
        "        Higher values = more conservative (requires higher energy)\n",
        "    min_speech_duration : float, default=0.3\n",
        "        Minimum duration (in seconds) of continuous high-energy frames\n",
        "        required to declare speech onset. This prevents false triggers\n",
        "        from transient noise spikes.\n",
        "    pre_speech_margin : float, default=0.5\n",
        "        Duration (in seconds) to preserve before detected onset.\n",
        "        Ensures natural transitions and context are not lost.\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    dict\n",
        "        onset_time : float or None\n",
        "            Time (in seconds) when speech is detected to begin.\n",
        "            None if detection fails.\n",
        "        trim_start : float or None\n",
        "            Recommended trim start time including pre-speech margin.\n",
        "            None if detection fails.\n",
        "        margin_used : float\n",
        "            Actual margin applied (may be less than requested if\n",
        "            onset occurs near file start)\n",
        "        success : bool\n",
        "            True if onset was successfully detected, False otherwise\n",
        "\n",
        "    Notes\n",
        "    -----\n",
        "    Detection methodology:\n",
        "    1. Compute RMS energy using 25ms frames with 10ms hop\n",
        "       (standard parameters for speech analysis)\n",
        "    2. Calculate adaptive energy threshold (20th percentile)\n",
        "    3. Find first sequence of frames above threshold lasting\n",
        "       at least min_speech_duration\n",
        "    4. Apply pre-speech margin to preserve natural onset\n",
        "\n",
        "    Failure cases:\n",
        "    - Audio too short for analysis\n",
        "    - No sustained high-energy segment found\n",
        "    - All frames have zero/invalid energy\n",
        "    \"\"\"\n",
        "    frame_length = int(0.025 * sr)  # 25ms frame\n",
        "    hop_length = int(0.010 * sr)     # 10ms hop\n",
        "\n",
        "    rms = librosa.feature.rms(y=y, frame_length=frame_length, hop_length=hop_length)[0]\n",
        "\n",
        "    if len(rms) == 0 or np.all(np.isnan(rms)) or np.all(rms == 0):\n",
        "        return {\n",
        "            'onset_time': None,\n",
        "            'trim_start': None,\n",
        "            'margin_used': 0.0,\n",
        "            'success': False\n",
        "        }\n",
        "\n",
        "    energy_threshold = np.percentile(rms, energy_threshold_percentile)\n",
        "    is_speech = rms > energy_threshold\n",
        "\n",
        "    min_frames = int(min_speech_duration * sr / hop_length)\n",
        "\n",
        "    if min_frames >= len(is_speech):\n",
        "        return {\n",
        "            'onset_time': None,\n",
        "            'trim_start': None,\n",
        "            'margin_used': 0.0,\n",
        "            'success': False\n",
        "        }\n",
        "\n",
        "    onset_time = None\n",
        "    found = False\n",
        "\n",
        "    for i in range(len(is_speech) - min_frames + 1):\n",
        "        if np.all(is_speech[i:i + min_frames]):\n",
        "            onset_time = librosa.frames_to_time(i, sr=sr, hop_length=hop_length)\n",
        "            found = True\n",
        "            break\n",
        "\n",
        "    if not found:\n",
        "        return {\n",
        "            'onset_time': None,\n",
        "            'trim_start': None,\n",
        "            'margin_used': 0.0,\n",
        "            'success': False\n",
        "        }\n",
        "\n",
        "    trim_start = max(0, onset_time - pre_speech_margin)\n",
        "    margin_used = onset_time - trim_start\n",
        "\n",
        "    return {\n",
        "        'onset_time': onset_time,\n",
        "        'trim_start': trim_start,\n",
        "        'margin_used': margin_used,\n",
        "        'success': True\n",
        "    }\n",
        "\n",
        "\n",
        "# ============================================================\n",
        "# DATASET ANALYSIS\n",
        "# ============================================================\n",
        "\n",
        "def analyze_audio_dataset_simple(df, audio_dir):\n",
        "    \"\"\"\n",
        "    Perform comprehensive analysis of audio dataset.\n",
        "\n",
        "    Computes per-file statistics and aggregate summary metrics for:\n",
        "    - Duration (total and effective after onset trimming)\n",
        "    - Speech onset timing\n",
        "    - Audio quality (silence ratio, RMS energy, clipping)\n",
        "    - Group-level comparisons\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    df : pd.DataFrame\n",
        "        Dataset metadata with required columns:\n",
        "        - FileName: audio file name\n",
        "        - label: binary label (0=Control, 1=Patient)\n",
        "        - Tipo soggetto: descriptive label (Controllo/Paziente)\n",
        "    audio_dir : str\n",
        "        Directory path containing audio files\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    stats_df : pd.DataFrame\n",
        "        Per-file statistics with columns:\n",
        "        - filename, label, tipo_soggetto\n",
        "        - sample_rate, duration_total, duration_after_trim\n",
        "        - onset_time, onset_detected, trim_amount\n",
        "        - rms_mean, rms_std, rms_max\n",
        "        - silence_ratio, silence_threshold\n",
        "        - zcr_mean (zero crossing rate)\n",
        "        - clipping_ratio, clipped_samples\n",
        "    summary : dict\n",
        "        Aggregate statistics organized by category:\n",
        "        - n_samples, n_controls, n_patients, n_errors\n",
        "        - sample_rate (min, max, unique)\n",
        "        - duration_total (mean, std, median, min, max, q25, q75)\n",
        "        - onset (mean, std, median, max, detection_success_rate)\n",
        "        - duration_after_trim (mean, std, median, min, max, q25, q75)\n",
        "        - quality (silence_ratio, rms, clipping metrics)\n",
        "        - by_group (separate statistics for controls and patients)\n",
        "    \"\"\"\n",
        "    audio_stats = []\n",
        "\n",
        "    print(\"=\"*80)\n",
        "    print(\"AUDIO DATASET ANALYSIS\")\n",
        "    print(\"=\"*80)\n",
        "    print(f\"Analyzing {len(df)} audio files...\")\n",
        "    print(f\"Onset detection: Energy-based adaptive threshold method\")\n",
        "    print(f\"Silence detection: 10% of peak energy method\\n\")\n",
        "\n",
        "    for idx, row in tqdm(df.iterrows(), total=len(df), desc=\"Processing\"):\n",
        "        audio_path = os.path.join(audio_dir, row['FileName'])\n",
        "\n",
        "        try:\n",
        "            y, sr = librosa.load(audio_path, sr=None, mono=True)\n",
        "\n",
        "            if len(y) == 0:\n",
        "                raise ValueError(\"Empty audio file\")\n",
        "\n",
        "            duration_total = len(y) / sr\n",
        "\n",
        "            # Clipping detection (amplitude near ±1.0)\n",
        "            clipping_threshold = 0.99\n",
        "            clipped_samples = np.sum(np.abs(y) >= clipping_threshold)\n",
        "            clipping_ratio = clipped_samples / len(y)\n",
        "\n",
        "            if clipping_ratio > 0.01:\n",
        "                print(f\"\\n  Warning: {row['FileName']}: {clipping_ratio:.1%} clipping detected\")\n",
        "\n",
        "            # Speech onset detection\n",
        "            onset = detect_speech_onset(y, sr)\n",
        "\n",
        "            if onset['success']:\n",
        "                onset_time = onset['onset_time']\n",
        "                trim_start = onset['trim_start']\n",
        "                duration_after_trim = duration_total - trim_start\n",
        "            else:\n",
        "                print(f\"\\n  Warning: {row['FileName']}: Onset detection failed\")\n",
        "                onset_time = None\n",
        "                trim_start = 0.0\n",
        "                duration_after_trim = duration_total\n",
        "\n",
        "            if duration_after_trim < 5.0:\n",
        "                print(f\"\\n  Warning: {row['FileName']}: Only {duration_after_trim:.1f}s after trim\")\n",
        "\n",
        "            # Audio quality metrics\n",
        "            rms = librosa.feature.rms(y=y)[0]\n",
        "\n",
        "            if len(rms) == 0 or np.all(np.isnan(rms)):\n",
        "                print(f\"\\n  Warning: {row['FileName']}: Invalid RMS values\")\n",
        "                rms = np.array([0.0])\n",
        "\n",
        "            rms_mean = np.mean(rms)\n",
        "            rms_std = np.std(rms)\n",
        "            rms_max = np.max(rms)\n",
        "\n",
        "            silence_ratio, silence_threshold = compute_silence_ratio(rms, method='adaptive')\n",
        "\n",
        "            zcr = librosa.feature.zero_crossing_rate(y)[0]\n",
        "            zcr_mean = np.mean(zcr)\n",
        "\n",
        "            audio_stats.append({\n",
        "                'filename': row['FileName'],\n",
        "                'label': row['label'],\n",
        "                'tipo_soggetto': row['Tipo soggetto'],\n",
        "                'sample_rate': sr,\n",
        "                'duration_total': duration_total,\n",
        "                'onset_time': onset_time,\n",
        "                'onset_detected': onset['success'],\n",
        "                'trim_amount': trim_start,\n",
        "                'duration_after_trim': duration_after_trim,\n",
        "                'rms_mean': rms_mean,\n",
        "                'rms_std': rms_std,\n",
        "                'rms_max': rms_max,\n",
        "                'silence_ratio': silence_ratio,\n",
        "                'silence_threshold': silence_threshold,\n",
        "                'zcr_mean': zcr_mean,\n",
        "                'clipping_ratio': clipping_ratio,\n",
        "                'clipped_samples': clipped_samples,\n",
        "                'samples_total': len(y)\n",
        "            })\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"\\n  Error processing {row['FileName']}: {e}\")\n",
        "            audio_stats.append({\n",
        "                'filename': row['FileName'],\n",
        "                'label': row['label'],\n",
        "                'tipo_soggetto': row['Tipo soggetto'],\n",
        "                'error': str(e)\n",
        "            })\n",
        "\n",
        "    stats_df = pd.DataFrame(audio_stats)\n",
        "    valid = stats_df.dropna(subset=['duration_total'])\n",
        "    onset_valid = valid[valid['onset_detected'] == True]\n",
        "\n",
        "    # Compute aggregate summary statistics\n",
        "    summary = {\n",
        "        'n_samples': len(valid),\n",
        "        'n_controls': (valid['label'] == 0).sum(),\n",
        "        'n_patients': (valid['label'] == 1).sum(),\n",
        "        'n_errors': len(stats_df) - len(valid),\n",
        "\n",
        "        'sample_rate': {\n",
        "            'min': int(valid['sample_rate'].min()),\n",
        "            'max': int(valid['sample_rate'].max()),\n",
        "            'unique': valid['sample_rate'].nunique()\n",
        "        },\n",
        "\n",
        "        'duration_total': {\n",
        "            'mean': valid['duration_total'].mean(),\n",
        "            'std': valid['duration_total'].std(),\n",
        "            'median': valid['duration_total'].median(),\n",
        "            'min': valid['duration_total'].min(),\n",
        "            'max': valid['duration_total'].max(),\n",
        "            'q25': valid['duration_total'].quantile(0.25),\n",
        "            'q75': valid['duration_total'].quantile(0.75)\n",
        "        },\n",
        "\n",
        "        'onset': {\n",
        "            'mean': onset_valid['onset_time'].mean() if len(onset_valid) > 0 else 0.0,\n",
        "            'std': onset_valid['onset_time'].std() if len(onset_valid) > 0 else 0.0,\n",
        "            'median': onset_valid['onset_time'].median() if len(onset_valid) > 0 else 0.0,\n",
        "            'max': onset_valid['onset_time'].max() if len(onset_valid) > 0 else 0.0,\n",
        "            'detection_success_rate': valid['onset_detected'].mean()\n",
        "        },\n",
        "\n",
        "        'duration_after_trim': {\n",
        "            'mean': valid['duration_after_trim'].mean(),\n",
        "            'std': valid['duration_after_trim'].std(),\n",
        "            'median': valid['duration_after_trim'].median(),\n",
        "            'min': valid['duration_after_trim'].min(),\n",
        "            'max': valid['duration_after_trim'].max(),\n",
        "            'q25': valid['duration_after_trim'].quantile(0.25),\n",
        "            'q75': valid['duration_after_trim'].quantile(0.75)\n",
        "        },\n",
        "\n",
        "        'quality': {\n",
        "            'silence_ratio_mean': valid['silence_ratio'].mean(),\n",
        "            'silence_ratio_std': valid['silence_ratio'].std(),\n",
        "            'rms_mean': valid['rms_mean'].mean(),\n",
        "            'rms_std': valid['rms_mean'].std(),\n",
        "            'clipping_mean': valid['clipping_ratio'].mean(),\n",
        "            'clipping_max': valid['clipping_ratio'].max(),\n",
        "            'n_clipped': (valid['clipping_ratio'] > 0.01).sum()\n",
        "        },\n",
        "\n",
        "        'by_group': {\n",
        "            'controls': {\n",
        "                'n': (valid['label'] == 0).sum(),\n",
        "                'duration_mean': valid[valid['label'] == 0]['duration_total'].mean(),\n",
        "                'duration_std': valid[valid['label'] == 0]['duration_total'].std(),\n",
        "                'duration_after_trim_mean': valid[valid['label'] == 0]['duration_after_trim'].mean(),\n",
        "                'onset_mean': onset_valid[onset_valid['label'] == 0]['onset_time'].mean() if len(onset_valid[onset_valid['label'] == 0]) > 0 else 0.0,\n",
        "                'silence_mean': valid[valid['label'] == 0]['silence_ratio'].mean(),\n",
        "                'clipping_mean': valid[valid['label'] == 0]['clipping_ratio'].mean()\n",
        "            },\n",
        "            'patients': {\n",
        "                'n': (valid['label'] == 1).sum(),\n",
        "                'duration_mean': valid[valid['label'] == 1]['duration_total'].mean(),\n",
        "                'duration_std': valid[valid['label'] == 1]['duration_total'].std(),\n",
        "                'duration_after_trim_mean': valid[valid['label'] == 1]['duration_after_trim'].mean(),\n",
        "                'onset_mean': onset_valid[onset_valid['label'] == 1]['onset_time'].mean() if len(onset_valid[onset_valid['label'] == 1]) > 0 else 0.0,\n",
        "                'silence_mean': valid[valid['label'] == 1]['silence_ratio'].mean(),\n",
        "                'clipping_mean': valid[valid['label'] == 1]['clipping_ratio'].mean()\n",
        "            }\n",
        "        }\n",
        "    }\n",
        "\n",
        "    return stats_df, summary\n",
        "\n",
        "\n",
        "# ============================================================\n",
        "# PROBLEMATIC FILE IDENTIFICATION\n",
        "# ============================================================\n",
        "\n",
        "def identify_problematic_audio(stats_df):\n",
        "    \"\"\"\n",
        "    Identify audio files with quality issues or insufficient content.\n",
        "\n",
        "    Files are flagged based on multiple quality criteria and assigned\n",
        "    a severity level (CRITICAL or WARNING) based on cumulative score.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    stats_df : pd.DataFrame\n",
        "        Per-file statistics from analyze_audio_dataset_simple()\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    pd.DataFrame\n",
        "        Problematic files sorted by severity, with columns:\n",
        "        - filename, tipo_soggetto\n",
        "        - severity: 'CRITICAL' or 'WARNING'\n",
        "        - severity_score: numerical score (higher = more severe)\n",
        "        - issues: concatenated description of all detected issues\n",
        "        - duration_total, duration_after_trim\n",
        "        - silence_ratio, rms_mean, clipping_ratio\n",
        "\n",
        "    Quality Criteria\n",
        "    ----------------\n",
        "    - Silence ratio > 60% (score +1) or > 80% (score +2)\n",
        "    - RMS energy < 0.01 (score +2) - very low volume\n",
        "    - Total duration < 60s (score +1)\n",
        "    - Duration after trim < 30s (score +2) - insufficient content\n",
        "    - Clipping ratio > 1% (score +1) or > 5% (score +2)\n",
        "    - Onset detection failed (score +1)\n",
        "\n",
        "    Severity Classification\n",
        "    -----------------------\n",
        "    - CRITICAL: severity_score >= 3 (multiple issues or severe single issue)\n",
        "    - WARNING: severity_score < 3 (minor or single issue)\n",
        "    \"\"\"\n",
        "    problematic = []\n",
        "\n",
        "    for idx, row in stats_df.iterrows():\n",
        "        if pd.isna(row.get('duration_total')):\n",
        "            problematic.append({\n",
        "                'filename': row['filename'],\n",
        "                'tipo_soggetto': row['tipo_soggetto'],\n",
        "                'severity': 'CRITICAL',\n",
        "                'severity_score': 10,\n",
        "                'issues': 'File loading failed',\n",
        "                'duration_total': np.nan,\n",
        "                'duration_after_trim': np.nan,\n",
        "                'silence_ratio': np.nan,\n",
        "                'rms_mean': np.nan,\n",
        "                'clipping_ratio': np.nan\n",
        "            })\n",
        "            continue\n",
        "\n",
        "        issues = []\n",
        "        severity_score = 0\n",
        "\n",
        "        if row['silence_ratio'] > 0.6:\n",
        "            issues.append(f\"High silence: {row['silence_ratio']:.1%}\")\n",
        "            severity_score += 2 if row['silence_ratio'] > 0.8 else 1\n",
        "\n",
        "        if row['rms_mean'] < 0.01:\n",
        "            issues.append(f\"Very low energy: {row['rms_mean']:.4f}\")\n",
        "            severity_score += 2\n",
        "\n",
        "        if row['duration_total'] < 60:\n",
        "            issues.append(f\"Short total: {row['duration_total']:.1f}s\")\n",
        "            severity_score += 1\n",
        "\n",
        "        if row['duration_after_trim'] < 30:\n",
        "            issues.append(f\"Short after trim: {row['duration_after_trim']:.1f}s\")\n",
        "            severity_score += 2\n",
        "\n",
        "        if row['clipping_ratio'] > 0.01:\n",
        "            issues.append(f\"Clipping: {row['clipping_ratio']:.1%}\")\n",
        "            severity_score += 2 if row['clipping_ratio'] > 0.05 else 1\n",
        "\n",
        "        if not row.get('onset_detected', True):\n",
        "            issues.append(\"Onset detection failed\")\n",
        "            severity_score += 1\n",
        "\n",
        "        if issues:\n",
        "            problematic.append({\n",
        "                'filename': row['filename'],\n",
        "                'tipo_soggetto': row['tipo_soggetto'],\n",
        "                'severity': 'CRITICAL' if severity_score >= 3 else 'WARNING',\n",
        "                'severity_score': severity_score,\n",
        "                'issues': ' | '.join(issues),\n",
        "                'duration_total': row['duration_total'],\n",
        "                'duration_after_trim': row['duration_after_trim'],\n",
        "                'silence_ratio': row['silence_ratio'],\n",
        "                'rms_mean': row['rms_mean'],\n",
        "                'clipping_ratio': row['clipping_ratio']\n",
        "            })\n",
        "\n",
        "    if problematic:\n",
        "        prob_df = pd.DataFrame(problematic)\n",
        "        prob_df = prob_df.sort_values('severity_score', ascending=False)\n",
        "        return prob_df\n",
        "    else:\n",
        "        return pd.DataFrame()\n",
        "\n",
        "\n",
        "# ============================================================\n",
        "# ONSET VALIDATION (VISUAL)\n",
        "# ============================================================\n",
        "\n",
        "def validate_onset_detection(df, audio_dir, stats_df, n_samples=4):\n",
        "    \"\"\"\n",
        "    Generate visual validation plots for onset detection results.\n",
        "\n",
        "    Creates waveform plots with marked onset times for manual inspection\n",
        "    and validation of the automatic detection algorithm.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    df : pd.DataFrame\n",
        "        Original dataset metadata\n",
        "    audio_dir : str\n",
        "        Directory containing audio files\n",
        "    stats_df : pd.DataFrame\n",
        "        Statistics from analysis (must contain onset detection results)\n",
        "    n_samples : int, default=4\n",
        "        Number of sample files to visualize\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    matplotlib.figure.Figure or None\n",
        "        Figure with n_samples subplots showing waveforms with onset markers.\n",
        "        Returns None if no valid files are available.\n",
        "\n",
        "    Notes\n",
        "    -----\n",
        "    - Attempts to balance samples (2 controls + 2 patients if possible)\n",
        "    - Only includes files where onset detection succeeded\n",
        "    - Red line: detected onset time\n",
        "    - Green line: trim start (onset - margin)\n",
        "    - Shaded region: portion that would be trimmed\n",
        "    \"\"\"\n",
        "    valid_files = stats_df[stats_df['onset_detected'] == True]\n",
        "\n",
        "    if len(valid_files) < n_samples:\n",
        "        n_samples = len(valid_files)\n",
        "\n",
        "    if n_samples == 0:\n",
        "        print(\"Warning: No valid files for onset validation\")\n",
        "        return None\n",
        "\n",
        "    controls = valid_files[valid_files['label'] == 0]\n",
        "    patients = valid_files[valid_files['label'] == 1]\n",
        "\n",
        "    samples = []\n",
        "    if len(controls) >= 2:\n",
        "        samples.extend(controls.sample(min(2, len(controls))).to_dict('records'))\n",
        "    if len(patients) >= 2:\n",
        "        samples.extend(patients.sample(min(2, len(patients))).to_dict('records'))\n",
        "\n",
        "    while len(samples) < n_samples:\n",
        "        remaining = valid_files[~valid_files['filename'].isin([s['filename'] for s in samples])]\n",
        "        if len(remaining) > 0:\n",
        "            samples.append(remaining.sample(1).to_dict('records')[0])\n",
        "        else:\n",
        "            break\n",
        "\n",
        "    fig, axes = plt.subplots(len(samples), 1, figsize=(15, 3*len(samples)))\n",
        "\n",
        "    if len(samples) == 1:\n",
        "        axes = [axes]\n",
        "\n",
        "    for ax, sample in zip(axes, samples):\n",
        "        audio_path = os.path.join(audio_dir, sample['filename'])\n",
        "\n",
        "        try:\n",
        "            y, sr = librosa.load(audio_path, sr=None, mono=True)\n",
        "            time = np.arange(len(y)) / sr\n",
        "\n",
        "            ax.plot(time, y, alpha=0.7, linewidth=0.5, color='steelblue')\n",
        "\n",
        "            if sample['onset_time'] is not None:\n",
        "                ax.axvline(sample['onset_time'], color='red', linestyle='--',\n",
        "                          linewidth=2, label=f\"Onset: {sample['onset_time']:.2f}s\")\n",
        "\n",
        "                ax.axvline(sample['trim_amount'], color='green', linestyle='--',\n",
        "                          linewidth=2, alpha=0.6, label=f\"Trim start: {sample['trim_amount']:.2f}s\")\n",
        "\n",
        "                ax.axvspan(0, sample['trim_amount'], alpha=0.2, color='red')\n",
        "\n",
        "            ax.set_title(f\"{sample['filename'][:50]} - {sample['tipo_soggetto']} - \"\n",
        "                        f\"Duration: {sample['duration_total']:.1f}s → {sample['duration_after_trim']:.1f}s after trim\")\n",
        "            ax.set_xlabel('Time (s)')\n",
        "            ax.set_ylabel('Amplitude')\n",
        "            ax.legend(loc='upper right')\n",
        "            ax.grid(True, alpha=0.3)\n",
        "\n",
        "        except Exception as e:\n",
        "            ax.text(0.5, 0.5, f\"Error loading: {e}\",\n",
        "                   ha='center', va='center', transform=ax.transAxes)\n",
        "\n",
        "    plt.tight_layout()\n",
        "    return fig\n",
        "\n",
        "\n",
        "# ============================================================\n",
        "# VISUALIZATION\n",
        "# ============================================================\n",
        "\n",
        "def plot_analysis(stats_df, summary):\n",
        "    \"\"\"\n",
        "    Create comprehensive 6-panel visualization of dataset statistics.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    stats_df : pd.DataFrame\n",
        "        Per-file statistics\n",
        "    summary : dict\n",
        "        Aggregate statistics\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    matplotlib.figure.Figure\n",
        "        6-panel figure with:\n",
        "        1. Duration by group (boxplot)\n",
        "        2. Duration distribution (histogram)\n",
        "        3. Onset time distribution (histogram)\n",
        "        4. Duration after trimming by group (boxplot)\n",
        "        5. Silence ratio by group (boxplot)\n",
        "        6. Duration vs Silence scatter (outlier detection)\n",
        "    \"\"\"\n",
        "    fig, axes = plt.subplots(2, 3, figsize=(18, 10))\n",
        "    fig.suptitle('Audio Dataset Analysis - Summary Statistics',\n",
        "                 fontsize=16, fontweight='bold')\n",
        "\n",
        "    valid = stats_df.dropna(subset=['duration_total'])\n",
        "\n",
        "    # Panel 1: Duration by group\n",
        "    ax = axes[0, 0]\n",
        "    valid.boxplot(column='duration_total', by='tipo_soggetto', ax=ax)\n",
        "    ax.axhline(y=30, color='red', linestyle='--', linewidth=1.5, alpha=0.7, label='30s')\n",
        "    ax.axhline(y=60, color='green', linestyle='--', linewidth=2, label='60s')\n",
        "    ax.axhline(y=90, color='orange', linestyle='--', linewidth=1.5, alpha=0.7, label='90s')\n",
        "    ax.set_title('Total Duration by Group')\n",
        "    ax.set_ylabel('Duration (seconds)')\n",
        "    ax.set_xlabel('')\n",
        "    ax.legend(loc='upper right', fontsize=8)\n",
        "    plt.sca(ax)\n",
        "    plt.xticks(rotation=0)\n",
        "\n",
        "    # Panel 2: Duration histogram\n",
        "    ax = axes[0, 1]\n",
        "    ax.hist(valid['duration_total'], bins=30, edgecolor='black', alpha=0.7, color='skyblue')\n",
        "    ax.axvline(summary['duration_total']['mean'], color='red', linestyle='--',\n",
        "               linewidth=2, label=f\"Mean: {summary['duration_total']['mean']:.1f}s\")\n",
        "    ax.axvline(summary['duration_total']['median'], color='green', linestyle='--',\n",
        "               linewidth=2, label=f\"Median: {summary['duration_total']['median']:.1f}s\")\n",
        "    ax.set_title('Duration Distribution')\n",
        "    ax.set_xlabel('Duration (seconds)')\n",
        "    ax.set_ylabel('Frequency')\n",
        "    ax.legend(fontsize=8)\n",
        "    ax.grid(True, alpha=0.3)\n",
        "\n",
        "    # Panel 3: Onset distribution\n",
        "    ax = axes[0, 2]\n",
        "    onset_valid = valid[valid['onset_detected'] == True]['onset_time'].dropna()\n",
        "    if len(onset_valid) > 0:\n",
        "        ax.hist(onset_valid, bins=30, edgecolor='black', alpha=0.7, color='coral')\n",
        "        ax.axvline(summary['onset']['mean'], color='red', linestyle='--',\n",
        "                   linewidth=2, label=f\"Mean: {summary['onset']['mean']:.2f}s\")\n",
        "        ax.axvline(summary['onset']['median'], color='green', linestyle='--',\n",
        "                   linewidth=2, label=f\"Median: {summary['onset']['median']:.2f}s\")\n",
        "        ax.legend(fontsize=8)\n",
        "    ax.set_title(f\"Speech Onset Time (detected in {summary['onset']['detection_success_rate']:.1%})\")\n",
        "    ax.set_xlabel('Onset Time (seconds)')\n",
        "    ax.set_ylabel('Frequency')\n",
        "    ax.grid(True, alpha=0.3)\n",
        "\n",
        "    # Panel 4: Duration after trim by group\n",
        "    ax = axes[1, 0]\n",
        "    valid.boxplot(column='duration_after_trim', by='tipo_soggetto', ax=ax)\n",
        "    ax.axhline(y=60, color='green', linestyle='--', linewidth=2, label='60s reference')\n",
        "    ax.axhline(y=30, color='red', linestyle='--', linewidth=1.5, alpha=0.7, label='30s reference')\n",
        "    ax.axhline(y=90, color='orange', linestyle='--', linewidth=1.5, label='90s reference')\n",
        "    ax.set_title('Duration After Onset Trimming')\n",
        "    ax.set_ylabel('Duration (seconds)')\n",
        "    ax.set_xlabel('')\n",
        "    ax.legend(loc='upper right', fontsize=8)\n",
        "    plt.sca(ax)\n",
        "    plt.xticks(rotation=0)\n",
        "\n",
        "    # Panel 5: Silence ratio by group\n",
        "    ax = axes[1, 1]\n",
        "    valid.boxplot(column='silence_ratio', by='tipo_soggetto', ax=ax)\n",
        "    ax.axhline(y=0.6, color='red', linestyle='--', linewidth=2,\n",
        "               label='60% reference')\n",
        "    ax.set_title('Silence Ratio by Group (10% of peak method)')\n",
        "    ax.set_ylabel('Silence Ratio')\n",
        "    ax.set_xlabel('')\n",
        "    ax.legend(loc='upper right', fontsize=8)\n",
        "    ax.set_ylim([0, 1])\n",
        "    plt.sca(ax)\n",
        "    plt.xticks(rotation=0)\n",
        "\n",
        "    # Panel 6: Duration vs Silence scatter\n",
        "    ax = axes[1, 2]\n",
        "    colors = {'Controllo': 'blue', 'Paziente': 'red'}\n",
        "    for tipo in valid['tipo_soggetto'].unique():\n",
        "        subset = valid[valid['tipo_soggetto'] == tipo]\n",
        "        ax.scatter(subset['duration_total'], subset['silence_ratio'],\n",
        "                  label=tipo, alpha=0.6, s=50, color=colors.get(tipo, 'gray'),\n",
        "                  edgecolors='black', linewidth=0.5)\n",
        "    ax.axhline(y=0.6, color='red', linestyle='--', linewidth=1.5, alpha=0.5)\n",
        "    ax.axvline(x=60, color='green', linestyle='--', linewidth=1.5, alpha=0.5)\n",
        "    ax.set_xlabel('Duration (seconds)')\n",
        "    ax.set_ylabel('Silence Ratio')\n",
        "    ax.set_title('Duration vs Silence (outlier detection)')\n",
        "    ax.legend()\n",
        "    ax.grid(True, alpha=0.3)\n",
        "\n",
        "    plt.tight_layout()\n",
        "    return fig\n",
        "\n",
        "\n",
        "# ============================================================\n",
        "# SUMMARY REPORT (CONSOLE OUTPUT)\n",
        "# ============================================================\n",
        "\n",
        "def print_summary_report(summary, problematic_df):\n",
        "    \"\"\"\n",
        "    Print formatted summary report to console.\n",
        "\n",
        "    Displays aggregate statistics organized by category:\n",
        "    - Dataset overview (sample counts by group)\n",
        "    - Sample rate consistency\n",
        "    - Duration statistics (total and after trimming)\n",
        "    - Speech onset detection performance\n",
        "    - Audio quality metrics\n",
        "    - Group comparisons (Controls vs Patients)\n",
        "    - Problematic file summary\n",
        "    \"\"\"\n",
        "    print(\"\\n\" + \"=\"*80)\n",
        "    print(\"SUMMARY REPORT\")\n",
        "    print(\"=\"*80)\n",
        "\n",
        "    print(f\"\\n Dataset Overview\")\n",
        "    print(f\"  Total samples:     {summary['n_samples']}\")\n",
        "    print(f\"  Controls:          {summary['n_controls']}\")\n",
        "    print(f\"  Patients:          {summary['n_patients']}\")\n",
        "    if summary['n_errors'] > 0:\n",
        "        print(f\"  Errors:            {summary['n_errors']}\")\n",
        "\n",
        "    print(f\"\\n Sample Rate\")\n",
        "    sr = summary['sample_rate']\n",
        "    if sr['unique'] == 1:\n",
        "        print(f\"  All files:         {sr['min']} Hz\")\n",
        "    else:\n",
        "        print(f\"  Range:             {sr['min']} - {sr['max']} Hz\")\n",
        "        print(f\"  Mixed rates:       {sr['unique']} different sample rates detected\")\n",
        "\n",
        "    print(f\"\\n Duration Statistics (Total)\")\n",
        "    d = summary['duration_total']\n",
        "    print(f\"  Mean ± SD:         {d['mean']:.1f}s ± {d['std']:.1f}s\")\n",
        "    print(f\"  Median:            {d['median']:.1f}s\")\n",
        "    print(f\"  Range:             [{d['min']:.1f}s, {d['max']:.1f}s]\")\n",
        "    print(f\"  Quartiles:         Q25={d['q25']:.1f}s, Q75={d['q75']:.1f}s\")\n",
        "\n",
        "    print(f\"\\n Speech Onset Detection\")\n",
        "    o = summary['onset']\n",
        "    print(f\"  Detection success: {o['detection_success_rate']:.1%}\")\n",
        "    print(f\"  Mean onset:        {o['mean']:.2f}s ± {o['std']:.2f}s\")\n",
        "    print(f\"  Median onset:      {o['median']:.2f}s\")\n",
        "    print(f\"  Max onset:         {o['max']:.2f}s\")\n",
        "\n",
        "    print(f\"\\n Duration After Trimming\")\n",
        "    dt = summary['duration_after_trim']\n",
        "    print(f\"  Mean ± SD:         {dt['mean']:.1f}s ± {dt['std']:.1f}s\")\n",
        "    print(f\"  Median:            {dt['median']:.1f}s\")\n",
        "    print(f\"  Range:             [{dt['min']:.1f}s, {dt['max']:.1f}s]\")\n",
        "    print(f\"  Quartiles:         Q25={dt['q25']:.1f}s, Q75={dt['q75']:.1f}s\")\n",
        "\n",
        "    print(f\"\\n Audio Quality\")\n",
        "    q = summary['quality']\n",
        "    print(f\"  Silence ratio:     {q['silence_ratio_mean']:.1%} ± {q['silence_ratio_std']:.1%}\")\n",
        "    print(f\"  RMS energy:        {q['rms_mean']:.4f} ± {q['rms_std']:.4f}\")\n",
        "    print(f\"  Clipping mean:     {q['clipping_mean']:.2%}\")\n",
        "    if q['n_clipped'] > 0:\n",
        "        print(f\"  Files with >1% clipping: {q['n_clipped']}\")\n",
        "\n",
        "    print(f\"\\n Group Comparison\")\n",
        "    ctrl = summary['by_group']['controls']\n",
        "    ptz = summary['by_group']['patients']\n",
        "\n",
        "    print(f\"  Sample size:\")\n",
        "    print(f\"    Controls:        {ctrl['n']}\")\n",
        "    print(f\"    Patients:        {ptz['n']}\")\n",
        "\n",
        "    print(f\"  Duration (total):\")\n",
        "    print(f\"    Controls:        {ctrl['duration_mean']:.1f}s ± {ctrl['duration_std']:.1f}s\")\n",
        "    print(f\"    Patients:        {ptz['duration_mean']:.1f}s ± {ptz['duration_std']:.1f}s\")\n",
        "\n",
        "    print(f\"  Duration (after trim):\")\n",
        "    print(f\"    Controls:        {ctrl['duration_after_trim_mean']:.1f}s\")\n",
        "    print(f\"    Patients:        {ptz['duration_after_trim_mean']:.1f}s\")\n",
        "\n",
        "    print(f\"  Silence ratio:\")\n",
        "    print(f\"    Controls:        {ctrl['silence_mean']:.1%}\")\n",
        "    print(f\"    Patients:        {ptz['silence_mean']:.1%}\")\n",
        "\n",
        "    if len(problematic_df) > 0:\n",
        "        print(f\"\\n⚠️  Problematic Audio Files\")\n",
        "        print(f\"  Total:             {len(problematic_df)}\")\n",
        "        print(f\"  Critical:          {(problematic_df['severity']=='CRITICAL').sum()}\")\n",
        "        print(f\"  Warnings:          {(problematic_df['severity']=='WARNING').sum()}\")\n",
        "    else:\n",
        "        print(f\"\\n✓ No problematic audio files detected\")\n",
        "\n",
        "    print(\"\\n\" + \"=\"*80)\n",
        "\n",
        "\n",
        "# ============================================================\n",
        "# SAVE TEXT REPORT\n",
        "# ============================================================\n",
        "\n",
        "def save_text_report(summary, problematic_df, output_path):\n",
        "    \"\"\"\n",
        "    Save comprehensive text report to file for documentation purposes.\n",
        "\n",
        "    Creates a formatted text file containing all aggregate statistics\n",
        "    suitable for inclusion in thesis or technical documentation.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    summary : dict\n",
        "        Aggregate statistics from analysis\n",
        "    problematic_df : pd.DataFrame\n",
        "        Problematic files report\n",
        "    output_path : str\n",
        "        Output file path (typically .txt)\n",
        "    \"\"\"\n",
        "    with open(output_path, 'w', encoding='utf-8') as f:\n",
        "        f.write(\"=\"*80 + \"\\n\")\n",
        "        f.write(\"AUDIO DATASET ANALYSIS REPORT\\n\")\n",
        "        f.write(\"=\"*80 + \"\\n\")\n",
        "        f.write(f\"\\nGenerated: {pd.Timestamp.now().strftime('%Y-%m-%d %H:%M:%S')}\\n\")\n",
        "\n",
        "        f.write(\"\\n\" + \"=\"*80 + \"\\n\")\n",
        "        f.write(\"DATASET OVERVIEW\\n\")\n",
        "        f.write(\"=\"*80 + \"\\n\")\n",
        "        f.write(f\"Total samples:     {summary['n_samples']}\\n\")\n",
        "        f.write(f\"Controls:          {summary['n_controls']}\\n\")\n",
        "        f.write(f\"Patients:          {summary['n_patients']}\\n\")\n",
        "        if summary['n_errors'] > 0:\n",
        "            f.write(f\"Errors:            {summary['n_errors']}\\n\")\n",
        "\n",
        "        f.write(\"\\n\" + \"=\"*80 + \"\\n\")\n",
        "        f.write(\"SAMPLE RATE\\n\")\n",
        "        f.write(\"=\"*80 + \"\\n\")\n",
        "        sr = summary['sample_rate']\n",
        "        if sr['unique'] == 1:\n",
        "            f.write(f\"All files:         {sr['min']} Hz\\n\")\n",
        "        else:\n",
        "            f.write(f\"Range:             {sr['min']} - {sr['max']} Hz\\n\")\n",
        "            f.write(f\"Mixed rates:       {sr['unique']} different sample rates\\n\")\n",
        "\n",
        "        f.write(\"\\n\" + \"=\"*80 + \"\\n\")\n",
        "        f.write(\"DURATION STATISTICS (Total)\\n\")\n",
        "        f.write(\"=\"*80 + \"\\n\")\n",
        "        d = summary['duration_total']\n",
        "        f.write(f\"Mean ± SD:         {d['mean']:.2f}s ± {d['std']:.2f}s\\n\")\n",
        "        f.write(f\"Median:            {d['median']:.2f}s\\n\")\n",
        "        f.write(f\"Range:             [{d['min']:.2f}s, {d['max']:.2f}s]\\n\")\n",
        "        f.write(f\"Q25:               {d['q25']:.2f}s\\n\")\n",
        "        f.write(f\"Q75:               {d['q75']:.2f}s\\n\")\n",
        "\n",
        "        f.write(\"\\n\" + \"=\"*80 + \"\\n\")\n",
        "        f.write(\"SPEECH ONSET DETECTION\\n\")\n",
        "        f.write(\"=\"*80 + \"\\n\")\n",
        "        o = summary['onset']\n",
        "        f.write(f\"Detection success: {o['detection_success_rate']:.1%}\\n\")\n",
        "        f.write(f\"Mean onset:        {o['mean']:.2f}s ± {o['std']:.2f}s\\n\")\n",
        "        f.write(f\"Median onset:      {o['median']:.2f}s\\n\")\n",
        "        f.write(f\"Max onset:         {o['max']:.2f}s\\n\")\n",
        "\n",
        "        f.write(\"\\n\" + \"=\"*80 + \"\\n\")\n",
        "        f.write(\"DURATION AFTER TRIMMING\\n\")\n",
        "        f.write(\"=\"*80 + \"\\n\")\n",
        "        dt = summary['duration_after_trim']\n",
        "        f.write(f\"Mean ± SD:         {dt['mean']:.2f}s ± {dt['std']:.2f}s\\n\")\n",
        "        f.write(f\"Median:            {dt['median']:.2f}s\\n\")\n",
        "        f.write(f\"Range:             [{dt['min']:.2f}s, {dt['max']:.2f}s]\\n\")\n",
        "        f.write(f\"Q25:               {dt['q25']:.2f}s\\n\")\n",
        "        f.write(f\"Q75:               {dt['q75']:.2f}s\\n\")\n",
        "\n",
        "        f.write(\"\\n\" + \"=\"*80 + \"\\n\")\n",
        "        f.write(\"AUDIO QUALITY METRICS\\n\")\n",
        "        f.write(\"=\"*80 + \"\\n\")\n",
        "        q = summary['quality']\n",
        "        f.write(f\"Silence ratio:     {q['silence_ratio_mean']:.2%} ± {q['silence_ratio_std']:.2%}\\n\")\n",
        "        f.write(f\"RMS energy:        {q['rms_mean']:.4f} ± {q['rms_std']:.4f}\\n\")\n",
        "        f.write(f\"Clipping mean:     {q['clipping_mean']:.3%}\\n\")\n",
        "        f.write(f\"Clipping max:      {q['clipping_max']:.3%}\\n\")\n",
        "        if q['n_clipped'] > 0:\n",
        "            f.write(f\"Files with >1% clipping: {q['n_clipped']}\\n\")\n",
        "\n",
        "        f.write(\"\\n\" + \"=\"*80 + \"\\n\")\n",
        "        f.write(\"GROUP COMPARISON (Controls vs Patients)\\n\")\n",
        "        f.write(\"=\"*80 + \"\\n\")\n",
        "        ctrl = summary['by_group']['controls']\n",
        "        ptz = summary['by_group']['patients']\n",
        "\n",
        "        f.write(f\"\\nSample Size:\\n\")\n",
        "        f.write(f\"  Controls:        {ctrl['n']}\\n\")\n",
        "        f.write(f\"  Patients:        {ptz['n']}\\n\")\n",
        "\n",
        "        f.write(f\"\\nDuration (total):\\n\")\n",
        "        f.write(f\"  Controls:        {ctrl['duration_mean']:.2f}s ± {ctrl['duration_std']:.2f}s\\n\")\n",
        "        f.write(f\"  Patients:        {ptz['duration_mean']:.2f}s ± {ptz['duration_std']:.2f}s\\n\")\n",
        "\n",
        "        f.write(f\"\\nDuration (after trim):\\n\")\n",
        "        f.write(f\"  Controls:        {ctrl['duration_after_trim_mean']:.2f}s\\n\")\n",
        "        f.write(f\"  Patients:        {ptz['duration_after_trim_mean']:.2f}s\\n\")\n",
        "\n",
        "        f.write(f\"\\nSilence ratio:\\n\")\n",
        "        f.write(f\"  Controls:        {ctrl['silence_mean']:.2%}\\n\")\n",
        "        f.write(f\"  Patients:        {ptz['silence_mean']:.2%}\\n\")\n",
        "\n",
        "        if len(problematic_df) > 0:\n",
        "            f.write(\"\\n\" + \"=\"*80 + \"\\n\")\n",
        "            f.write(\"PROBLEMATIC FILES\\n\")\n",
        "            f.write(\"=\"*80 + \"\\n\")\n",
        "            f.write(f\"Total:             {len(problematic_df)}\\n\")\n",
        "            f.write(f\"Critical:          {(problematic_df['severity']=='CRITICAL').sum()}\\n\")\n",
        "            f.write(f\"Warnings:          {(problematic_df['severity']=='WARNING').sum()}\\n\")\n",
        "\n",
        "        f.write(\"\\n\" + \"=\"*80 + \"\\n\")\n",
        "        f.write(\"END OF REPORT\\n\")\n",
        "        f.write(\"=\"*80 + \"\\n\")\n",
        "\n",
        "\n",
        "# ============================================================\n",
        "# EXECUTE ANALYSIS\n",
        "# ============================================================\n",
        "\n",
        "if RUN_DATASET_ANALYSIS:\n",
        "    # Define output directory for dataset analysis\n",
        "    output_dir = os.path.join(DATA_PATH, 'dataset_free_speech_analysis')\n",
        "    os.makedirs(output_dir, exist_ok=True)\n",
        "\n",
        "    # Run comprehensive analysis\n",
        "    stats_df, summary = analyze_audio_dataset_simple(df, CONFIG['audio_dir'])\n",
        "\n",
        "    # Identify problematic files\n",
        "    problematic_df = identify_problematic_audio(stats_df)\n",
        "\n",
        "    # Display summary report in console\n",
        "    print_summary_report(summary, problematic_df)\n",
        "\n",
        "    # Generate and save visualization\n",
        "    fig = plot_analysis(stats_df, summary)\n",
        "    plot_path = os.path.join(output_dir, 'audio_analysis.png')\n",
        "    plt.savefig(plot_path, dpi=300, bbox_inches='tight')\n",
        "    plt.show()\n",
        "\n",
        "    # Visual validation of onset detection\n",
        "    print(\"\\n\" + \"=\"*80)\n",
        "    print(\"ONSET DETECTION VALIDATION\")\n",
        "    print(\"=\"*80)\n",
        "    print(\"Generating visual validation for 4 random samples...\")\n",
        "\n",
        "    val_fig = validate_onset_detection(df, CONFIG['audio_dir'], stats_df, n_samples=4)\n",
        "    if val_fig is not None:\n",
        "        val_path = os.path.join(output_dir, 'onset_validation.png')\n",
        "        plt.savefig(val_path, dpi=150, bbox_inches='tight')\n",
        "        plt.show()\n",
        "        print(f\"Visual validation saved: {val_path}\")\n",
        "\n",
        "    # Save outputs\n",
        "    csv_path = os.path.join(output_dir, 'audio_statistics.csv')\n",
        "    stats_df.to_csv(csv_path, index=False)\n",
        "\n",
        "    report_path = os.path.join(output_dir, 'audio_analysis_report.txt')\n",
        "    save_text_report(summary, problematic_df, report_path)\n",
        "\n",
        "    if len(problematic_df) > 0:\n",
        "        prob_path = os.path.join(output_dir, 'problematic_audio.csv')\n",
        "        problematic_df.to_csv(prob_path, index=False)\n",
        "\n",
        "    # Final summary\n",
        "    print(\"\\n\" + \"=\"*80)\n",
        "    print(\"OUTPUT FILES\")\n",
        "    print(\"=\"*80)\n",
        "    print(f\"Per-file statistics (CSV):    {csv_path}\")\n",
        "    print(f\"Summary report (text):        {report_path}\")\n",
        "    print(f\"Visualization (6-panel):      {plot_path}\")\n",
        "    if val_fig is not None:\n",
        "        print(f\"Onset validation (visual):    {val_path}\")\n",
        "    if len(problematic_df) > 0:\n",
        "        print(f\"Problematic files (CSV):      {prob_path}\")\n",
        "\n",
        "    print(\"\\nAnalysis complete.\")\n",
        "\n",
        "else:\n",
        "    print(\"-> Skipping dataset analysis (RUN_DATASET_ANALYSIS = False)\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lqmTqCB734j0"
      },
      "source": [
        "## xAI"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CaVVpau47Q7G"
      },
      "outputs": [],
      "source": [
        "# ============================================================\n",
        "# EXPLAINABLE AI - SETUP\n",
        "# ============================================================\n",
        "# ImplementazionE di 2 tecniche complementari:\n",
        "# 1. Integrated Gradients (Sundararajan et al. 2017)\n",
        "# 2. Attention Rollout (Abnar & Zuidema 2020)\n",
        "# ============================================================\n",
        "\n",
        "import os\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from tqdm.auto import tqdm\n",
        "import librosa\n",
        "import librosa.display\n",
        "from scipy import stats\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Setup plotting style\n",
        "plt.style.use('seaborn-v0_8-darkgrid')\n",
        "sns.set_palette(\"husl\")\n",
        "\n",
        "print(\"✓ Setup completato\")\n",
        "\n",
        "# ============================================================\n",
        "# MODIFICA AL MODELLO: Estrazione Attention Weights\n",
        "# ============================================================\n",
        "# Aggiungiamo la capacità di estrarre attention da tutti i layer\n",
        "# SENZA modificare i pesi (solo inference mode)\n",
        "# ============================================================\n",
        "\n",
        "class HuBERTClassifierExplainable(nn.Module):\n",
        "    \"\"\"\n",
        "    Versione modificata di HuBERTClassifier per Explainable AI.\n",
        "\n",
        "    Modifiche rispetto all'originale:\n",
        "    - Flag return_all_attentions per estrarre attention da tutti i 12 layer HuBERT\n",
        "    - Flag return_embeddings per estrarre hidden states intermedi\n",
        "    - Nessuna modifica ai pesi o all'architettura\n",
        "\n",
        "    Uso:\n",
        "    - Training: usa forward() normale (comportamento identico)\n",
        "    - XAI: usa forward(..., return_all_attentions=True, return_embeddings=True)\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, model_name, freeze_layers=9, hidden_dim=256,\n",
        "                 dropout=0.3, num_classes=2, pooling_type='attention'):\n",
        "        super().__init__()\n",
        "\n",
        "        # HuBERT encoder\n",
        "        self.hubert = HubertModel.from_pretrained(model_name)\n",
        "        self.hidden_size = self.hubert.config.hidden_size  # 768\n",
        "\n",
        "        # Freeze primi N layer (come training)\n",
        "        for layer_idx in range(freeze_layers):\n",
        "            for param in self.hubert.encoder.layers[layer_idx].parameters():\n",
        "                param.requires_grad = False\n",
        "\n",
        "        # Pooling layer\n",
        "        self.pooling_type = pooling_type\n",
        "        if pooling_type == 'attention':\n",
        "            self.pooling = AttentionPooling(self.hidden_size)\n",
        "\n",
        "        # Classifier MLP\n",
        "        self.classifier = nn.Sequential(\n",
        "            nn.Dropout(dropout),\n",
        "            nn.Linear(self.hidden_size, hidden_dim),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(dropout),\n",
        "            nn.Linear(hidden_dim, num_classes)\n",
        "        )\n",
        "\n",
        "    def forward(self, input_values, attention_mask=None,\n",
        "                return_all_attentions=False, return_embeddings=False):\n",
        "        \"\"\"\n",
        "        Forward pass con opzioni per Explainability.\n",
        "\n",
        "        Args:\n",
        "            input_values: audio waveform (batch, time)\n",
        "            attention_mask: mask per padding (batch, time)\n",
        "            return_all_attentions: se True, restituisce attention da tutti layer HuBERT\n",
        "            return_embeddings: se True, restituisce hidden states\n",
        "\n",
        "        Returns:\n",
        "            Se return_all_attentions=False e return_embeddings=False:\n",
        "                logits: (batch, num_classes)\n",
        "\n",
        "            Se return_all_attentions=True o return_embeddings=True:\n",
        "                dict con chiavi:\n",
        "                - 'logits': (batch, num_classes)\n",
        "                - 'hubert_attentions': tuple di 12 tensori (batch, heads, time, time) [opzionale]\n",
        "                - 'pooling_attention': (batch, time, 1) [se attention pooling]\n",
        "                - 'embeddings': (batch, time, 768) [opzionale]\n",
        "        \"\"\"\n",
        "        # HuBERT encoding con opzione di estrarre attention\n",
        "        outputs = self.hubert(\n",
        "            input_values,\n",
        "            attention_mask=attention_mask,\n",
        "            output_attentions=return_all_attentions,  # ← Chiave: registra attention\n",
        "            output_hidden_states=return_embeddings\n",
        "        )\n",
        "\n",
        "        embeddings = outputs.last_hidden_state  # (batch, time, 768)\n",
        "\n",
        "        # Pooling\n",
        "        if self.pooling_type == 'attention':\n",
        "            pooled, attn_weights = self.pooling(embeddings)\n",
        "        else:\n",
        "            pooled = embeddings.mean(dim=1)\n",
        "            attn_weights = None\n",
        "\n",
        "        # Classificazione\n",
        "        logits = self.classifier(pooled)\n",
        "\n",
        "        # Return diversificato in base a flag\n",
        "        if return_all_attentions or return_embeddings:\n",
        "            result = {'logits': logits}\n",
        "\n",
        "            if return_all_attentions:\n",
        "                result['hubert_attentions'] = outputs.attentions  # Tuple di 12 tensori\n",
        "                if attn_weights is not None:\n",
        "                    result['pooling_attention'] = attn_weights\n",
        "\n",
        "            if return_embeddings:\n",
        "                result['embeddings'] = embeddings\n",
        "                result['pooled'] = pooled\n",
        "\n",
        "            return result\n",
        "        else:\n",
        "            return logits\n",
        "\n",
        "print(\"✓ HuBERTClassifierExplainable definito\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PgwlHGsp7UvQ"
      },
      "source": [
        "## IG"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Yb9gix_A7W64"
      },
      "outputs": [],
      "source": [
        "# TECHNIQUE 1: INTEGRATED GRADIENTS\n",
        "# Paper: Sundararajan et al. (2017) \"Axiomatic Attribution for Deep Networks\"\n",
        "\n",
        "\n",
        "import os\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "import librosa\n",
        "import librosa.display\n",
        "import matplotlib.pyplot as plt\n",
        "from transformers import Wav2Vec2FeatureExtractor\n",
        "\n",
        "\n",
        "\n",
        "class IntegratedGradients:\n",
        "    \"\"\"\n",
        "    Integrated Gradients per attribution.\n",
        "\n",
        "    Formula:\n",
        "        IG(x) = (x - baseline) × ∫₀¹ ∂F(baseline + α(x - baseline))/∂x dα\n",
        "    \"\"\"\n",
        "\n",
        "\n",
        "    def __init__(self, model, device, onset_map=None):\n",
        "        self.model = model\n",
        "        self.device = device\n",
        "        self.onset_map = onset_map if onset_map is not None else {}\n",
        "        self.model.eval()  # SEMPRE in eval mode\n",
        "\n",
        "\n",
        "    def compute_attributions(\n",
        "        self,\n",
        "        inputaudio,\n",
        "        targetclass,\n",
        "        nsteps=50,\n",
        "        internalbatchsize=1,\n",
        "        baseline=None,\n",
        "    ):\n",
        "        inputaudio = inputaudio.to(self.device)\n",
        "\n",
        "\n",
        "        if baseline is None:\n",
        "            baseline = torch.zeros_like(inputaudio, device=self.device)\n",
        "        else:\n",
        "            baseline = baseline.to(self.device)\n",
        "\n",
        "\n",
        "        alpha_power = 3.0  # più grande => più punti vicino a 0\n",
        "        alphas = torch.linspace(0.0, 1.0, steps=nsteps + 1, device=self.device) ** alpha_power\n",
        "\n",
        "\n",
        "        all_grads = []\n",
        "        for i in range(0, len(alphas), internalbatchsize):\n",
        "            batch_alphas = alphas[i : i + internalbatchsize]  # [b]\n",
        "            interpolated = baseline + batch_alphas.view(-1, 1) * (inputaudio - baseline)  # [b, T]\n",
        "\n",
        "            interpolated = interpolated.detach().requires_grad_(True)\n",
        "\n",
        "\n",
        "            logits = self.model(interpolated)  # [b, C]\n",
        "            target = logits[:, targetclass].sum()  # scalare\n",
        "\n",
        "\n",
        "            grads = torch.autograd.grad(\n",
        "                outputs=target,\n",
        "                inputs=interpolated,\n",
        "                create_graph=False\n",
        "            )[0]  # [b, T]\n",
        "            all_grads.append(grads.detach())\n",
        "\n",
        "\n",
        "            if torch.isnan(grads).any() or torch.isinf(grads).any():\n",
        "                print(\"GRADS NAN/INF! step index:\", i, \"alpha range:\", batch_alphas[0].item(), \"->\", batch_alphas[-1].item())\n",
        "                print(\"interpolated min/max:\", interpolated.min().item(), interpolated.max().item())\n",
        "\n",
        "\n",
        "            del interpolated, logits, target, grads\n",
        "\n",
        "\n",
        "        all_grads = torch.cat(all_grads, dim=0)  # [nsteps+1, T]\n",
        "\n",
        "\n",
        "        # Trapezoid rule PESATA (necessaria perché alphas non sono uniformi)\n",
        "        dalpha = (alphas[1:] - alphas[:-1]).view(-1, 1)              # (n_steps, 1)\n",
        "        trap = (all_grads[:-1] + all_grads[1:]) / 2.0               # (n_steps, T)\n",
        "        integral = (trap * dalpha).sum(dim=0, keepdim=True)         # (1, T)\n",
        "\n",
        "\n",
        "        print(\"alpha[1]=\", alphas[1].item(), \"alpha[2]=\", alphas[2].item(), \"alpha[10]=\", alphas[10].item())\n",
        "\n",
        "\n",
        "        attributions = (inputaudio - baseline) * integral           # (1, T)\n",
        "\n",
        "\n",
        "        # Completeness check\n",
        "        with torch.no_grad():\n",
        "            out_in = self.model(inputaudio)[0, targetclass]\n",
        "            out_base = self.model(baseline)[0, targetclass]\n",
        "            delta_out = out_in - out_base\n",
        "            sum_attr = attributions.sum()\n",
        "            completeness_error_abs = (delta_out - sum_attr).abs().item()\n",
        "\n",
        "\n",
        "            print(\n",
        "                f\"logit_input={out_in.item():.4f}  logit_base={out_base.item():.4f}  delta_out={delta_out.item():.4f}\"\n",
        "            )\n",
        "            print(f\"sum_attr={sum_attr.item():.4f}  abs_err={completeness_error_abs:.4f}\")\n",
        "\n",
        "\n",
        "            completeness_error_rel = completeness_error_abs / (delta_out.abs().item() + 1e-8)\n",
        "\n",
        "\n",
        "        return attributions.detach().cpu(), completeness_error_abs, completeness_error_rel\n",
        "\n",
        "\n",
        "    def visualize_on_spectrogram(self, audio_path, target_class, filename, sr=16000, save_dir=None):\n",
        "        # Carica audio\n",
        "        waveform, _ = librosa.load(audio_path, sr=sr, mono=True)\n",
        "\n",
        "\n",
        "        # STEP 1: ONSET TRIMMING (se hai onsetmap)\n",
        "        if (filename + \".wav\") in self.onset_map:\n",
        "            trim_amount_sec = self.onset_map[filename + \".wav\"]\n",
        "            trim_samples = int(trim_amount_sec * sr)\n",
        "            if 0 < trim_samples < len(waveform):\n",
        "                waveform = waveform[trim_samples:]\n",
        "                print(f\"Trimmed: {trim_amount_sec:.2f}s removed\")\n",
        "\n",
        "\n",
        "        # STEP 2: TRUNCATION a 30s\n",
        "        max_duration = 30\n",
        "        max_samples = sr * max_duration\n",
        "        original_length = len(waveform)\n",
        "        if len(waveform) > max_samples:\n",
        "            waveform = waveform[:max_samples]\n",
        "            print(f\"Truncated: {len(waveform)/sr:.2f}s (was {original_length/sr:.2f}s)\")\n",
        "\n",
        "\n",
        "        # STEP 3: Processor\n",
        "        processor = Wav2Vec2FeatureExtractor.from_pretrained(\"facebook/hubert-base-ls960\")\n",
        "        inputs = processor(\n",
        "            waveform,\n",
        "            sampling_rate=sr,\n",
        "            return_tensors=\"pt\",\n",
        "            padding=True\n",
        "        )\n",
        "        input_values = inputs.input_values.to(self.device)  # [1, T]\n",
        "        waveform_processed = input_values.squeeze(0).detach().cpu().numpy()\n",
        "\n",
        "\n",
        "        baseline_wave = np.zeros_like(waveform)\n",
        "        baseline_inputs = processor(baseline_wave, sampling_rate=sr, return_tensors=\"pt\", padding=True)\n",
        "        baseline_values = baseline_inputs.input_values.to(self.device)\n",
        "\n",
        "\n",
        "        print(f\"Baseline creata (shape: {tuple(baseline_values.shape)})\")\n",
        "\n",
        "\n",
        "        print(\"input dtype:\", input_values.dtype, \"device:\", input_values.device)\n",
        "        print(\"model dtype:\", next(self.model.parameters()).dtype, \"device:\", next(self.model.parameters()).device)\n",
        "\n",
        "\n",
        "        # DEBUG: controlla se il percorso baseline->input è smooth\n",
        "        alphas_test = [0.0, 1e-6, 1e-4, 1e-2, 0.1, 0.5, 1.0]\n",
        "        with torch.no_grad():\n",
        "            for a in alphas_test:\n",
        "                x = baseline_values + a * (input_values - baseline_values)\n",
        "                li = self.model(x)[0, target_class].item()\n",
        "                print(f\"alpha={a:.0e}  logit={li:.6f}\")\n",
        "\n",
        "\n",
        "        # Predizione e confidenza\n",
        "        with torch.no_grad():\n",
        "            logits = self.model(input_values)\n",
        "            probs = torch.softmax(logits, dim=1)\n",
        "            confidence = probs[0, target_class].item()\n",
        "\n",
        "\n",
        "        # Scelta adattiva n_steps e internal_batch_size\n",
        "        if confidence > 0.95:\n",
        "            n_steps = 600\n",
        "            internal_batch_size = 1\n",
        "            print(f\"High confidence ({confidence:.3f}) -> n_steps={n_steps}, batch=1\")\n",
        "        elif confidence > 0.85:\n",
        "            n_steps = 500\n",
        "            internal_batch_size = 1\n",
        "            print(f\"Medium-high confidence ({confidence:.3f}) -> n_steps={n_steps}, batch=1\")\n",
        "        else:\n",
        "            n_steps = 30\n",
        "            internal_batch_size = 2\n",
        "            print(f\"Normal confidence ({confidence:.3f}) -> n_steps={n_steps}, batch=2\")\n",
        "\n",
        "\n",
        "        # Integrated Gradients\n",
        "        attributions, completeness_err_abs, completeness_err_rel = self.compute_attributions(\n",
        "            input_values,\n",
        "            target_class,\n",
        "            nsteps=n_steps,\n",
        "            internalbatchsize=internal_batch_size,\n",
        "            baseline=baseline_values,\n",
        "        )\n",
        "\n",
        "\n",
        "        # Mel-spectrogram (da waveform processato)\n",
        "        mel_spec = librosa.feature.melspectrogram(\n",
        "            y=waveform_processed,\n",
        "            sr=sr,\n",
        "            n_mels=128,\n",
        "            hop_length=512,\n",
        "            n_fft=2048\n",
        "        )\n",
        "        mel_spec_db = librosa.power_to_db(mel_spec, ref=np.max)\n",
        "\n",
        "\n",
        "        # Resample attributions alla risoluzione temporale del mel\n",
        "        n_frames = mel_spec.shape[1]\n",
        "\n",
        "\n",
        "        # Robustezza shape: vogliamo [1, T]\n",
        "        if attributions.dim() == 1:\n",
        "            attributions = attributions.unsqueeze(0)  # [1, T]\n",
        "        elif attributions.dim() != 2:\n",
        "            raise ValueError(f\"attributions shape non supportata: {tuple(attributions.shape)}\")\n",
        "\n",
        "\n",
        "        # Per interpolate(linear) serve [N, C, L]\n",
        "        attr_3d = attributions.unsqueeze(1)  # [1, 1, T]\n",
        "        attribution_resampled = F.interpolate(\n",
        "            attr_3d,\n",
        "            size=n_frames,\n",
        "            mode=\"linear\",\n",
        "            align_corners=False\n",
        "        ).squeeze(0).squeeze(0).detach().cpu().numpy()  # [n_frames]\n",
        "\n",
        "\n",
        "        # Espandi su asse frequenza (replica su tutte le bande: è una mappa time-only)\n",
        "        attribution_map = np.tile(attribution_resampled, (mel_spec.shape[0], 1))\n",
        "\n",
        "\n",
        "        # Scala robusta per visualizzazione: evita z-score per-sample (ingannevole),\n",
        "        # ma limita outlier con percentili e mantiene il significato del valore (segno e ampiezza).\n",
        "        p_low, p_high = np.percentile(attribution_resampled, [1, 99])\n",
        "        lim = float(max(abs(p_low), abs(p_high)) + 1e-12)  # +eps per evitare lim=0\n",
        "\n",
        "        # VISUALIZATION\n",
        "        fig, axes = plt.subplots(2, 1, figsize=(16, 8))\n",
        "        duration = input_values.shape[1] / sr\n",
        "\n",
        "        # Plot 1\n",
        "        img1 = librosa.display.specshow(\n",
        "            mel_spec_db,\n",
        "            sr=sr,\n",
        "            hop_length=512,\n",
        "            x_axis=\"time\",\n",
        "            y_axis=\"mel\",\n",
        "            ax=axes[0],\n",
        "            cmap=\"viridis\"\n",
        "        )\n",
        "        axes[0].set_title(f\"Mel-Spectrogram - {filename}\", fontsize=13, fontweight=\"bold\")\n",
        "        axes[0].set_ylabel(\"Mel Frequency\", fontsize=11)\n",
        "        fig.colorbar(img1, ax=axes[0], format=\"%+2.0f dB\")\n",
        "\n",
        "        # Plot 2\n",
        "        img2 = axes[1].imshow(\n",
        "            attribution_map,\n",
        "            aspect=\"auto\",\n",
        "            cmap=\"RdBu_r\",\n",
        "            extent=[0, duration, 0, sr/2],\n",
        "            vmin=-lim,\n",
        "            vmax=lim,\n",
        "            origin=\"lower\",\n",
        "            interpolation=\"bilinear\"\n",
        "        )\n",
        "\n",
        "        axes[1].set_title(\n",
        "            f'Integrated Gradients Attribution - Temporal Profile (Class: {\"Paziente\" if target_class==1 else \"Controllo\"})',\n",
        "            fontsize=13,\n",
        "            fontweight=\"bold\"\n",
        "        )\n",
        "        axes[1].set_xlabel(\"Time (s)\", fontsize=11)\n",
        "\n",
        "        # nascondi scala Y\n",
        "        axes[1].set_yticks([])\n",
        "        axes[1].set_ylabel(\"\")\n",
        "\n",
        "        fig.colorbar(img2, ax=axes[1], label=\"Attribution\")\n",
        "\n",
        "        plt.tight_layout()\n",
        "\n",
        "\n",
        "        # Save\n",
        "        if save_dir:\n",
        "            os.makedirs(save_dir, exist_ok=True)\n",
        "            save_path = os.path.join(save_dir, f\"IG_{filename}.png\")\n",
        "            plt.savefig(save_path, dpi=300, bbox_inches=\"tight\")\n",
        "            print(f\"Salvato: {save_path}\")\n",
        "\n",
        "        time_axis = np.linspace(0, duration, len(attribution_resampled))\n",
        "\n",
        "        metrics = {\n",
        "            \"completeness_error_abs\": completeness_err_abs,\n",
        "            \"completeness_error_rel\": completeness_err_rel,\n",
        "            \"mean_attribution\": float(attribution_resampled.mean()),\n",
        "            \"std_attribution\": float(attribution_resampled.std()),\n",
        "            \"max_attribution_time\": float(time_axis[np.argmax(attribution_resampled)]),\n",
        "            \"positive_ratio\": float((attribution_resampled > 0).mean()),\n",
        "        }\n",
        "\n",
        "\n",
        "        return fig, attribution_map, metrics\n",
        "\n",
        "\n",
        "print(\"IntegratedGradients definito\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q_aCCrE07dqA"
      },
      "source": [
        "## Attention Rollout"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZvIv2BIS7fkz"
      },
      "outputs": [],
      "source": [
        "#\n",
        "# TECHNIQUE 2: ATTENTION ROLLOUT\n",
        "#\n",
        "# Paper: Abnar & Zuidema (2020) \"Quantifying Attention Flow in Transformers\"\n",
        "#\n",
        "\n",
        "class AttentionRollout:\n",
        "    \"\"\"\n",
        "    Attention Rollout per tracciare flusso informazione temporale.\n",
        "\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, model, device, onset_map=None):\n",
        "        self.model = model\n",
        "        self.device = device\n",
        "        self.onset_map = onset_map if onset_map is not None else {}\n",
        "        self.model.eval()\n",
        "\n",
        "    def compute_rollout(self, input_audio, head_fusion=\"mean\", discard_ratio=0.0):\n",
        "        \"\"\"\n",
        "        Calcola Attention Rollout.\n",
        "\n",
        "        Args:\n",
        "            input_audio: waveform (1, time)\n",
        "            head_fusion: 'mean', 'max', 'min' - come aggregare multi-head\n",
        "            discard_ratio: percentuale di attention minime da scartare (0-0.2)\n",
        "\n",
        "        Returns:\n",
        "            rollout: (time,) - attention score per timestep\n",
        "            layer_attentions: list di attention per layer (debugging)\n",
        "        \"\"\"\n",
        "        input_audio = input_audio.to(self.device)\n",
        "\n",
        "        # Forward con estrazione attention\n",
        "        with torch.no_grad():\n",
        "            outputs = self.model(\n",
        "                input_audio,\n",
        "                return_all_attentions=True,\n",
        "                return_embeddings=False\n",
        "            )\n",
        "\n",
        "        # Estrae attention weights da HuBERT\n",
        "        hubert_attentions = outputs['hubert_attentions']  # Tuple di 12 tensori\n",
        "        # Ogni tensore: (batch=1, num_heads, time, time)\n",
        "\n",
        "        # Estrae attention pooling finale\n",
        "        pooling_attention = outputs.get('pooling_attention', None)\n",
        "        # Shape: (batch=1, time, 1)\n",
        "\n",
        "        #\n",
        "        # STEP 1: Fuse multi-head attention per ciascun layer\n",
        "        #\n",
        "        fused_attentions = []\n",
        "\n",
        "        for layer_idx, attn in enumerate(hubert_attentions):\n",
        "            attn = attn.squeeze(0)  # (num_heads, time, time)\n",
        "\n",
        "            # Aggregazione heads\n",
        "            if head_fusion == \"mean\":\n",
        "                attn_fused = attn.mean(dim=0)  # (time, time)\n",
        "            elif head_fusion == \"max\":\n",
        "                attn_fused = attn.max(dim=0)[0]\n",
        "            elif head_fusion == \"min\":\n",
        "                attn_fused = attn.min(dim=0)[0]\n",
        "            else:\n",
        "                raise ValueError(f\"Unknown head_fusion: {head_fusion}\")\n",
        "\n",
        "            # Discard low attention (optional, paper usa 0.1)\n",
        "            if discard_ratio > 0:\n",
        "                threshold = attn_fused.flatten().kthvalue(\n",
        "                    int(attn_fused.numel() * discard_ratio)\n",
        "                )[0]\n",
        "                attn_fused = attn_fused.clamp(min=threshold)\n",
        "\n",
        "            # Normalizza righe (somma = 1)\n",
        "            attn_fused = attn_fused / (attn_fused.sum(dim=-1, keepdim=True) + 1e-8)\n",
        "\n",
        "            fused_attentions.append(attn_fused)\n",
        "\n",
        "        #\n",
        "        # STEP 2: Rollout attraverso layer HuBERT\n",
        "        #\n",
        "        # Formula: A_rollout = (I + Aᵢ)/2\n",
        "        # Implementazione ricorsiva:\n",
        "        # R_L = (I + A_L)/2\n",
        "        # R_{l-1} = (I + A_{l-1})/2 × R_l\n",
        "        #\n",
        "        num_layers = len(fused_attentions)\n",
        "        time_dim = fused_attentions[0].shape[0]\n",
        "\n",
        "        # Matrice identità\n",
        "        I = torch.eye(time_dim, device=self.device)\n",
        "\n",
        "        # Inizia dall'ultimo layer: R_L = (I + A_L)/2\n",
        "        rollout = (I + fused_attentions[-1]) / 2.0\n",
        "\n",
        "        # Propaga all'indietro attraverso i layer\n",
        "        for layer_idx in range(num_layers - 2, -1, -1):\n",
        "            # A_normalized = (I + A_i)/2\n",
        "            A_normalized = (I + fused_attentions[layer_idx]) / 2.0\n",
        "\n",
        "            # Rollout: R_i-1 = A_normalized × R_i\n",
        "            rollout = torch.matmul(A_normalized, rollout)\n",
        "\n",
        "            # Normalizza per stabilità numerica\n",
        "            rollout = rollout / (rollout.sum(dim=-1, keepdim=True) + 1e-8)\n",
        "\n",
        "        #\n",
        "        # STEP 3: Include Attention Pooling finale (CORRETTO)\n",
        "        #\n",
        "        # rollout: (time, time) - quanto ogni token contribuisce a ogni altro\n",
        "        # pooling_attention: (time, 1) - quanto ogni token contribuisce al pooled vector\n",
        "        #\n",
        "        # CORREZIONE: Usa prodotto matriciale, NON element-wise\n",
        "        # rollout_scores = pooling_attention^T × rollout → (1, time) × (time, time)\n",
        "        # Semplificato: somma pesata delle colonne\n",
        "        #\n",
        "        if pooling_attention is not None:\n",
        "            pooling_attention = pooling_attention.squeeze()  # (time,)\n",
        "\n",
        "            # Prodotto matriciale corretto: (1, time) × (time, time) = (1, time)\n",
        "            # Poi somma su colonne per ottenere importance score finale\n",
        "            rollout_scores = torch.matmul(pooling_attention.unsqueeze(0), rollout).squeeze()\n",
        "            # Shape: (time,)\n",
        "\n",
        "            # Normalizza a [0, 1]\n",
        "            rollout_scores = rollout_scores / (rollout_scores.sum() + 1e-8)\n",
        "        else:\n",
        "            # Fallback: media su colonne\n",
        "            rollout_scores = rollout.mean(dim=0)\n",
        "            rollout_scores = rollout_scores / (rollout_scores.sum() + 1e-8)\n",
        "\n",
        "        return rollout_scores.cpu().numpy(), fused_attentions\n",
        "\n",
        "    def visualize_rollout(self, audio_path, filename, sr=16000, save_dir=None):\n",
        "        \"\"\"\n",
        "        Visualizza Attention Rollout con overlay su waveform e spectrogram.\n",
        "        \"\"\"\n",
        "        # Carica audio\n",
        "        waveform, _ = librosa.load(audio_path, sr=sr, mono=True)\n",
        "\n",
        "        # STEP 1: ONSET TRIMMING (se hai onsetmap)\n",
        "        if hasattr(self, 'onset_map') and filename + '.wav' in self.onset_map:\n",
        "            trim_amount_sec = self.onset_map[filename + '.wav']\n",
        "            trim_samples = int(trim_amount_sec * sr)\n",
        "            if 0 < trim_samples < len(waveform):\n",
        "                waveform = waveform[trim_samples:]\n",
        "                print(f\"Trimmed: {trim_amount_sec:.2f}s removed\")\n",
        "\n",
        "        # STEP 2: TRUNCATION a 30s\n",
        "        max_duration = 30  # Stesso di CONFIG['max_duration']\n",
        "        max_samples = sr * max_duration\n",
        "        original_length = len(waveform)\n",
        "        if len(waveform) > max_samples:\n",
        "            waveform = waveform[:max_samples]\n",
        "            print(f\"Truncated: {len(waveform)/sr:.2f}s (was {original_length/sr:.2f}s)\")\n",
        "\n",
        "        # STEP 3: Processor (per padding se necessario)\n",
        "        from transformers import Wav2Vec2FeatureExtractor\n",
        "        processor = Wav2Vec2FeatureExtractor.from_pretrained('facebook/hubert-base-ls960')\n",
        "\n",
        "        inputs = processor(waveform, sampling_rate=sr, return_tensors=\"pt\", padding=True)\n",
        "        input_values = inputs.input_values.to(self.device)\n",
        "\n",
        "        # CORRETTO: Estrai waveform PROCESSATO\n",
        "        waveform_processed = input_values.squeeze(0).cpu().numpy()\n",
        "\n",
        "        # Compute rollout\n",
        "        rollout_scores, _ = self.compute_rollout(input_values, head_fusion=\"mean\")\n",
        "\n",
        "        #\n",
        "        # VISUALIZATION\n",
        "        #\n",
        "        fig, axes = plt.subplots(3, 1, figsize=(16, 10))\n",
        "\n",
        "        # Asse temporale da tensor processato\n",
        "        duration = input_values.shape[1] / sr\n",
        "        time_axis = np.linspace(0, duration, len(rollout_scores))\n",
        "\n",
        "        # Plot 1: Waveform con overlay attention\n",
        "        waveform_time = np.linspace(0, duration, len(waveform_processed))\n",
        "        axes[0].plot(waveform_time, waveform_processed, linewidth=0.5, color='black', alpha=0.7)\n",
        "        axes[0].set_ylabel('Amplitude', fontsize=11)\n",
        "        axes[0].set_title(f'Waveform - {filename}', fontsize=13, fontweight='bold')\n",
        "        axes[0].set_xlim(0, duration)\n",
        "        axes[0].grid(True, alpha=0.3)\n",
        "\n",
        "        # Overlay attention (area colorata)\n",
        "        ax_twin = axes[0].twinx()\n",
        "        ax_twin.fill_between(time_axis, 0, rollout_scores, alpha=0.4, color='red', label='Attention Rollout')\n",
        "        ax_twin.set_ylabel('Attention Weight', fontsize=11, color='red')\n",
        "        ax_twin.tick_params(axis='y', labelcolor='red')\n",
        "        ax_twin.set_ylim(0, rollout_scores.max() * 1.2)\n",
        "        ax_twin.legend(loc='upper right')\n",
        "\n",
        "        # Plot 2: Mel-spectrogram con overlay\n",
        "        mel_spec = librosa.feature.melspectrogram(y=waveform_processed, sr=sr, n_mels=128, hop_length=512)\n",
        "        mel_spec_db = librosa.power_to_db(mel_spec, ref=np.max)\n",
        "\n",
        "        img = librosa.display.specshow(\n",
        "            mel_spec_db, sr=sr, hop_length=512,\n",
        "            x_axis='time', y_axis='mel', ax=axes[1], cmap='viridis'\n",
        "        )\n",
        "        axes[1].set_title('Mel-Spectrogram with Attention Overlay', fontsize=13, fontweight='bold')\n",
        "        fig.colorbar(img, ax=axes[1], format='%+2.0f dB')\n",
        "\n",
        "        # Overlay attention come contorno\n",
        "        rollout_resampled = np.interp(\n",
        "            np.linspace(0, duration, mel_spec.shape[1]),\n",
        "            time_axis,\n",
        "            rollout_scores\n",
        "        )\n",
        "\n",
        "        axes[1].plot(np.linspace(0, duration, len(rollout_resampled)),\n",
        "                    rollout_resampled * sr / 4,  # Scale per visualizzazione\n",
        "                    color='red', linewidth=3, alpha=0.8, label='Attention Rollout')\n",
        "        axes[1].legend(loc='upper right')\n",
        "\n",
        "        # Plot 3: Attention profile temporale dettagliato\n",
        "        axes[2].plot(time_axis, rollout_scores, linewidth=2, color='darkred', marker='o',\n",
        "                    markersize=3, alpha=0.7)\n",
        "        axes[2].fill_between(time_axis, 0, rollout_scores, alpha=0.3, color='red')\n",
        "        axes[2].set_title('Attention Rollout: Temporal Importance Profile',\n",
        "                         fontsize=13, fontweight='bold')\n",
        "        axes[2].set_xlabel('Time (s)', fontsize=11)\n",
        "        axes[2].set_ylabel('Attention Weight', fontsize=11)\n",
        "        axes[2].grid(True, alpha=0.3)\n",
        "        axes[2].set_xlim(0, duration)\n",
        "\n",
        "        # Highlight top-k timesteps\n",
        "        topk = 5\n",
        "        top_indices = np.argsort(rollout_scores)[-topk:]\n",
        "        for idx in top_indices:\n",
        "            axes[2].axvline(time_axis[idx], color='orange', linestyle='--',\n",
        "                          alpha=0.5, linewidth=1.5)\n",
        "        axes[2].text(0.02, 0.95, f'Top-{topk} timesteps highlighted',\n",
        "                    transform=axes[2].transAxes, fontsize=10,\n",
        "                    verticalalignment='top',\n",
        "                    bbox=dict(boxstyle='round', facecolor='wheat', alpha=0.5))\n",
        "\n",
        "        plt.tight_layout()\n",
        "\n",
        "        # Save\n",
        "        if save_dir:\n",
        "            os.makedirs(save_dir, exist_ok=True)\n",
        "            save_path = os.path.join(save_dir, f'Rollout_{filename}.png')\n",
        "            plt.savefig(save_path, dpi=300, bbox_inches='tight')\n",
        "            print(f\"   Salvato: {save_path}\")\n",
        "\n",
        "        #\n",
        "        # METRICHE PER INTERPRETAZIONE\n",
        "        #\n",
        "        metrics = {\n",
        "            'entropy': stats.entropy(rollout_scores + 1e-8),  # Dispersione attention\n",
        "            'max_attention_time': time_axis[np.argmax(rollout_scores)],\n",
        "            'top5_times': time_axis[top_indices].tolist(),\n",
        "            'attention_concentration': rollout_scores.max() / rollout_scores.mean(),\n",
        "        }\n",
        "\n",
        "        return fig, rollout_scores, metrics\n",
        "\n",
        "print('AttentionRollout definito')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qwckPean7tKD"
      },
      "source": [
        "## Recupero validation set fold 4"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hcC7TYTE98Ze"
      },
      "outputs": [],
      "source": [
        "# ============================================================\n",
        "# CONFIGURAZIONE PATH OUTPUT\n",
        "# ============================================================\n",
        "# Verifica che paths sia già definito (dal setup esperimento)\n",
        "# Se non esiste, lo crea ora\n",
        "# ============================================================\n",
        "\n",
        "if RUN_XAI:\n",
        "    # Path del modello fold 4\n",
        "    MODEL_PATH = os.path.join(paths['models'], 'final_model', 'best_model_fold4.pth')\n",
        "\n",
        "    # Directory output per XAI (sottocartella di results)\n",
        "    OUTPUT_DIR = os.path.join(paths['results'], 'explainability')\n",
        "\n",
        "    # Crea directory se non esiste\n",
        "    os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
        "\n",
        "    print(\"=\"*60)\n",
        "    print(\"CONFIGURAZIONE PATH XAI\")\n",
        "    print(\"=\"*60)\n",
        "    print(f\"Modello: {MODEL_PATH}\")\n",
        "    print(f\"Output:  {OUTPUT_DIR}\")\n",
        "    print(\"=\"*60)\n",
        "    print()\n",
        "\n",
        "    # Verifica che il modello esista\n",
        "    if not os.path.exists(MODEL_PATH):\n",
        "        raise FileNotFoundError(f\"Modello non trovato: {MODEL_PATH}\")\n",
        "\n",
        "    print(\"✓ Path verificati\\n\")\n",
        "else:\n",
        "    print(\"- Skipping XAI paths setup: RUN_XAI = False\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uVz7yu1k7CWs"
      },
      "outputs": [],
      "source": [
        "# ============================================================\n",
        "# LOAD MODEL + GET PREDICTIONS ON VALIDATION SET FOLD\n",
        "# ============================================================\n",
        "#\n",
        "#\n",
        "# - Il modello selezionato DEVE usare il suo validation fold\n",
        "# ============================================================\n",
        "\n",
        "if RUN_XAI:\n",
        "    # ====================\n",
        "    # STEP 1: Ottieni predizioni del modello su validation set\n",
        "    # ====================\n",
        "\n",
        "\n",
        "    def get_predictions_on_valset(model, val_df, processor, device, config):\n",
        "        \"\"\"\n",
        "        Ottieni predizioni del modello su validation set.\n",
        "\n",
        "        Returns:\n",
        "            val_df con colonne aggiunte: pred_label, confidence, correct\n",
        "        \"\"\"\n",
        "        model.eval()\n",
        "\n",
        "        predictions = []\n",
        "        confidences = []\n",
        "\n",
        "        for idx, row in tqdm(val_df.iterrows(), total=len(val_df), desc=\"Predizioni validation\"):\n",
        "            audio_path = os.path.join(config['audio_dir'], row['FileName'])\n",
        "\n",
        "            # Carica audio\n",
        "            waveform, _ = librosa.load(audio_path, sr=16000, mono=True)\n",
        "\n",
        "            # Applica stesso preprocessing di SpeechDataset\n",
        "\n",
        "            # Estrai filename PRIMA di usarlo\n",
        "            filename = row['FileName']\n",
        "            audio_path = os.path.join(config['audio_dir'], filename)\n",
        "\n",
        "            # STEP 1: ONSET TRIMMING (se hai onsetmap)\n",
        "            if onset_map is not None and filename in onset_map:\n",
        "                trim_amount_sec = onset_map[filename]\n",
        "                trim_samples = int(trim_amount_sec * 16000)\n",
        "                if 0 < trim_samples < len(waveform):\n",
        "                    waveform = waveform[trim_samples:]\n",
        "                    print(f\"   🔧 Trimmed: {trim_amount_sec:.2f}s removed\")\n",
        "\n",
        "            # STEP 2: TRUNCATION a 30s\n",
        "            max_duration = 30  # Stesso di CONFIG['max_duration']\n",
        "            max_samples = 16000 * max_duration\n",
        "            original_length = len(waveform)\n",
        "            if len(waveform) > max_samples:\n",
        "                waveform = waveform[:max_samples]\n",
        "                print(f\"   ✂️  Truncated: {len(waveform)/16000:.2f}s (was {original_length/16000:.2f}s)\")\n",
        "\n",
        "            # STEP 3: Processor\n",
        "            from transformers import Wav2Vec2FeatureExtractor\n",
        "            processor = Wav2Vec2FeatureExtractor.from_pretrained('facebook/hubert-base-ls960')\n",
        "\n",
        "            inputs = processor(\n",
        "                waveform,\n",
        "                sampling_rate=16000,\n",
        "                return_tensors=\"pt\",\n",
        "                padding=True\n",
        "            )\n",
        "            input_values = inputs.input_values.to(device)\n",
        "\n",
        "            # Predizione\n",
        "            with torch.no_grad():\n",
        "                logits = model(inputs['input_values'].to(device))\n",
        "                probs = F.softmax(logits, dim=1)\n",
        "                pred_label = logits.argmax(dim=1).item()\n",
        "                confidence = probs[0, pred_label].item()\n",
        "\n",
        "            predictions.append(pred_label)\n",
        "            confidences.append(confidence)\n",
        "\n",
        "        val_df['pred_label'] = predictions\n",
        "        val_df['confidence'] = confidences\n",
        "        val_df['correct'] = (val_df['label'] == val_df['pred_label'])\n",
        "\n",
        "        return val_df\n",
        "\n",
        "\n",
        "    # ====================\n",
        "    # Carica modello\n",
        "    # ====================\n",
        "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "    model = HuBERTClassifierExplainable(\n",
        "        model_name=CONFIG['model_name'],\n",
        "        freeze_layers=CONFIG['freeze_layers'],\n",
        "        hidden_dim=CONFIG['hidden_dim'],\n",
        "        dropout=CONFIG['dropout'],\n",
        "        num_classes=CONFIG['num_classes'],\n",
        "        pooling_type=CONFIG['pooling_type']\n",
        "    ).to(device)\n",
        "\n",
        "    MODEL_PATH = os.path.join(paths['models'], 'final_model', 'best_model_fold4.pth')\n",
        "\n",
        "    checkpoint = torch.load(MODEL_PATH, map_location=device)\n",
        "    model.load_state_dict(checkpoint['model_state_dict'])\n",
        "\n",
        "    print(f\"✓ Modello caricato: {MODEL_PATH}\")\n",
        "\n",
        "    # ====================\n",
        "    # Recupera validation set FOLD 4\n",
        "    # ====================\n",
        "    # Usa StratifiedKFold con STESSO seed per ricostruire split identico\n",
        "\n",
        "    from sklearn.model_selection import StratifiedKFold\n",
        "\n",
        "    skf = StratifiedKFold(n_splits=CONFIG['n_folds'], shuffle=True, random_state=CONFIG['seed'])\n",
        "\n",
        "    # Itera fino a fold 4\n",
        "    for fold_num, (train_idx, val_idx) in enumerate(skf.split(df, df['label']), 1):\n",
        "        if fold_num == 4:\n",
        "            val_df_fold4 = df.iloc[val_idx].reset_index(drop=True)\n",
        "            break\n",
        "\n",
        "    print(f\"\\n✓ Validation set Fold 4 recuperato:\")\n",
        "    print(f\"   Totale campioni: {len(val_df_fold4)}\")\n",
        "    print(f\"   Controlli: {(val_df_fold4['label']==0).sum()}\")\n",
        "    print(f\"   Pazienti: {(val_df_fold4['label']==1).sum()}\")\n",
        "\n",
        "    # ====================\n",
        "    # Ottieni predizioni\n",
        "    # ====================\n",
        "    val_df_fold4 = get_predictions_on_valset(model, val_df_fold4, processor, device, CONFIG)\n",
        "\n",
        "\n",
        "    # ====================\n",
        "    # STEP 2: Analisi delle predizioni\n",
        "    # ====================\n",
        "    print(\"\\n\" + \"=\"*60)\n",
        "    print(\"ANALISI PREDIZIONI VALIDATION SET FOLD 4\")\n",
        "    print(\"=\"*60)\n",
        "\n",
        "    print(f\"\\nAccuracy: {val_df_fold4['correct'].mean():.3f}\")\n",
        "    print(f\"Confidenza media: {val_df_fold4['confidence'].mean():.3f}\")\n",
        "\n",
        "    print(\"\\n--- BREAKDOWN PER CLASSE ---\")\n",
        "    for label in [0, 1]:\n",
        "        label_name = \"Controllo\" if label == 0 else \"Paziente\"\n",
        "        subset = val_df_fold4[val_df_fold4['label'] == label]\n",
        "        correct = subset['correct'].sum()\n",
        "        total = len(subset)\n",
        "\n",
        "        print(f\"\\n{label_name} (n={total}):\")\n",
        "        print(f\"  Corretti: {correct}/{total} ({correct/total*100:.1f}%)\")\n",
        "        print(f\"  Confidenza media: {subset['confidence'].mean():.3f}\")\n",
        "\n",
        "\n",
        "    # ====================\n",
        "    # STEP 3: Selezione strategica campioni per XAI\n",
        "    # ====================\n",
        "    print(\"\\n\" + \"=\"*60)\n",
        "    print(\"SELEZIONE CAMPIONI PER XAI\")\n",
        "    print(\"=\"*60)\n",
        "\n",
        "    # Strategia:\n",
        "    # - 2 Controlli corretti (alta confidenza)\n",
        "    # - 2 Pazienti corretti (alta confidenza)\n",
        "    # - 1 Errore (qualunque classe, per failure mode analysis)\n",
        "\n",
        "    selected_samples = []\n",
        "\n",
        "    # 1. Controlli corretti (top 2 per confidenza)\n",
        "    controls_correct = val_df_fold4[\n",
        "        (val_df_fold4['label'] == 0) & (val_df_fold4['correct'] == True)\n",
        "    ].nlargest(2, 'confidence')\n",
        "\n",
        "    for idx, row in controls_correct.iterrows():\n",
        "        selected_samples.append({\n",
        "            'path': os.path.join(CONFIG['audio_dir'], row['FileName']),\n",
        "            'label': int(row['label']),\n",
        "            'pred_label': int(row['pred_label']),\n",
        "            'confidence': row['confidence'],\n",
        "            'id': f\"Control_correct_{row['FileName'].replace('.wav', '')}\",\n",
        "            'filename': row['FileName']\n",
        "        })\n",
        "\n",
        "    print(f\"\\n✓ Selezionati {len(controls_correct)} Controlli corretti\")\n",
        "    for s in selected_samples[-2:]:\n",
        "        print(f\"   - {s['filename']} (conf: {s['confidence']:.3f})\")\n",
        "\n",
        "    # 2. Pazienti corretti (top 2 per confidenza)\n",
        "    patients_correct = val_df_fold4[\n",
        "        (val_df_fold4['label'] == 1) & (val_df_fold4['correct'] == True)\n",
        "    ].nlargest(2, 'confidence')\n",
        "\n",
        "    for idx, row in patients_correct.iterrows():\n",
        "        selected_samples.append({\n",
        "            'path': os.path.join(CONFIG['audio_dir'], row['FileName']),\n",
        "            'label': int(row['label']),\n",
        "            'pred_label': int(row['pred_label']),\n",
        "            'confidence': row['confidence'],\n",
        "            'id': f\"Patient_correct_{row['FileName'].replace('.wav', '')}\",\n",
        "            'filename': row['FileName']\n",
        "        })\n",
        "\n",
        "    print(f\"\\n✓ Selezionati {len(patients_correct)} Pazienti corretti\")\n",
        "    for s in selected_samples[-2:]:\n",
        "        print(f\"   - {s['filename']} (conf: {s['confidence']:.3f})\")\n",
        "\n",
        "    # 3. Errori (max 1, quello con confidenza più alta = errore più \"sicuro\")\n",
        "    errors = val_df_fold4[val_df_fold4['correct'] == False]\n",
        "\n",
        "    if len(errors) > 0:\n",
        "        error_sample = errors.nlargest(1, 'confidence').iloc[0]\n",
        "        selected_samples.append({\n",
        "            'path': os.path.join(CONFIG['audio_dir'], error_sample['FileName']),\n",
        "            'label': int(error_sample['label']),\n",
        "            'pred_label': int(error_sample['pred_label']),\n",
        "            'confidence': error_sample['confidence'],\n",
        "            'id': f\"Error_{error_sample['FileName'].replace('.wav', '')}\",\n",
        "            'filename': error_sample['FileName']\n",
        "        })\n",
        "        print(f\"\\n✓ Selezionato 1 errore per failure mode analysis\")\n",
        "        print(f\"   - {error_sample['FileName']} (true: {error_sample['label']}, pred: {error_sample['pred_label']}, conf: {error_sample['confidence']:.3f})\")\n",
        "    else:\n",
        "        print(\"\\n⚠️ Nessun errore trovato (accuracy 100%)\")\n",
        "\n",
        "    print(f\"\\n{'='*60}\")\n",
        "    print(f\"TOTALE CAMPIONI SELEZIONATI: {len(selected_samples)}\")\n",
        "    print(f\"{'='*60}\")\n",
        "\n",
        "    # Salva selezione\n",
        "    selected_df = pd.DataFrame(selected_samples)\n",
        "    selected_csv_path = os.path.join(OUTPUT_DIR, 'selected_samples.csv')\n",
        "    selected_df.to_csv(selected_csv_path, index=False)\n",
        "    print(f\"\\n✓ Selezione salvata: {OUTPUT_DIR}/selected_samples.csv\")\n",
        "else:\n",
        "    print(\"- Skipping XAI validation set loading: RUN_XAI = False\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "34Vzlybw_qP7"
      },
      "source": [
        "## Esecuzione"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a0EKWLiu_fff"
      },
      "outputs": [],
      "source": [
        "# ============================================================\n",
        "# EXPLAINABLE AI - ESECUZIONE COMPLETA\n",
        "# ============================================================\n",
        "# Applica 2 tecniche su 5 campioni selezionati:\n",
        "# 1. Integrated Gradients\n",
        "# 2. Attention Rollout\n",
        "#\n",
        "# ============================================================\n",
        "\n",
        "\n",
        "# ============================================================\n",
        "# ESECUZIONE PIPELINE\n",
        "# ============================================================\n",
        "\n",
        "if RUN_XAI:\n",
        "    import gc\n",
        "\n",
        "    print(\"\\n\" + \"=\"*60)\n",
        "    print(\"INIZIO ANALISI EXPLAINABLE AI\")\n",
        "    print(\"=\"*60)\n",
        "    print(f\"Campioni: {len(selected_samples)}\")\n",
        "    print(f\"Tecniche: 2 (IG, Rollout)\")\n",
        "    print(f\"Output: {OUTPUT_DIR}\")\n",
        "    print(\"=\"*60)\n",
        "\n",
        "\n",
        "    # Inizializza tecniche XAI\n",
        "    print(\"\\nInizializzazione tecniche XAI...\")\n",
        "    ig = IntegratedGradients(model, device, onset_map=onset_map)\n",
        "    rollout = AttentionRollout(model, device, onset_map=onset_map)\n",
        "    print(\"✓ Tecniche inizializzate\\n\")\n",
        "\n",
        "\n",
        "    # Storage risultati\n",
        "    all_results = []\n",
        "\n",
        "\n",
        "    # ============================================================\n",
        "    # ANALISI PER CIASCUN SAMPLE\n",
        "    # ============================================================\n",
        "\n",
        "\n",
        "    for idx, sample in enumerate(selected_samples, 1):\n",
        "        # GESTIONE MEMORIA: Clear cache prima di ogni campione\n",
        "        torch.cuda.empty_cache()\n",
        "        gc.collect()\n",
        "\n",
        "        audio_path = sample['path']\n",
        "        true_label = sample['label']\n",
        "        pred_label = sample['pred_label']\n",
        "        confidence = sample['confidence']\n",
        "        sample_id = sample['id']\n",
        "        filename = sample['filename']\n",
        "\n",
        "\n",
        "        print(f\"\\n{'='*60}\")\n",
        "        print(f\"[{idx}/{len(selected_samples)}] Analisi: {filename}\")\n",
        "        print(f\"{'='*60}\")\n",
        "        print(f\"Label reale: {'Paziente' if true_label==1 else 'Controllo'}\")\n",
        "        print(f\"Predizione:  {'Paziente' if pred_label==1 else 'Controllo'}\")\n",
        "        print(f\"Confidenza:  {confidence:.3f}\")\n",
        "        print(f\"Corretto:    {'✓' if true_label==pred_label else '✗'}\")\n",
        "        print(\"-\"*60)\n",
        "\n",
        "\n",
        "        # Target class per XAI: usa predizione del modello\n",
        "        target_class = pred_label\n",
        "\n",
        "\n",
        "        # ============================================================\n",
        "        # 1. INTEGRATED GRADIENTS\n",
        "        # ============================================================\n",
        "        print(\"\\n[1/2] Integrated Gradients...\")\n",
        "        try:\n",
        "            fig_ig, _, metrics_ig = ig.visualize_on_spectrogram(\n",
        "                audio_path,\n",
        "                target_class,\n",
        "                filename=filename.replace('.wav', ''),\n",
        "                save_dir=os.path.join(OUTPUT_DIR, 'integrated_gradients')\n",
        "            )\n",
        "            plt.close(fig_ig)\n",
        "            print(\"      ✓ Completato\")\n",
        "\n",
        "            # CLEAR memoria dopo IG\n",
        "            del fig_ig\n",
        "            torch.cuda.empty_cache()\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"      ✗ Errore: {str(e)}\")\n",
        "            metrics_ig = {'completeness_error': None, 'mean_attribution': None, 'max_attribution_time': None}\n",
        "\n",
        "        # Clear aggressivo tra tecniche\n",
        "        torch.cuda.empty_cache()\n",
        "        gc.collect()\n",
        "\n",
        "\n",
        "        # ============================================================\n",
        "        # 2. ATTENTION ROLLOUT\n",
        "        # ============================================================\n",
        "        print(\"[2/2] Attention Rollout...\")\n",
        "        try:\n",
        "            fig_rollout, _, metrics_rollout = rollout.visualize_rollout(\n",
        "                audio_path,\n",
        "                filename=filename.replace('.wav', ''),\n",
        "                save_dir=os.path.join(OUTPUT_DIR, 'attention_rollout')\n",
        "            )\n",
        "            plt.close(fig_rollout)\n",
        "            print(\"      ✓ Completato\")\n",
        "\n",
        "            # CLEAR memoria dopo Rollout\n",
        "            del fig_rollout\n",
        "            torch.cuda.empty_cache()\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"      ✗ Errore: {str(e)}\")\n",
        "            metrics_rollout = {'entropy': None, 'max_attention_time': None, 'attention_concentration': None}\n",
        "\n",
        "        # Clear aggressivo tra tecniche\n",
        "        torch.cuda.empty_cache()\n",
        "        gc.collect()\n",
        "\n",
        "\n",
        "        # ============================================================\n",
        "        # ============================================================\n",
        "        # AGGREGAZIONE RISULTATI\n",
        "        # ============================================================\n",
        "        result = {\n",
        "            'sample_id': sample_id,\n",
        "            'filename': filename,\n",
        "            'true_label': 'Paziente' if true_label == 1 else 'Controllo',\n",
        "            'pred_label': 'Paziente' if pred_label == 1 else 'Controllo',\n",
        "            'confidence': confidence,\n",
        "            'correct': true_label == pred_label,\n",
        "\n",
        "\n",
        "            # IG metrics\n",
        "            'ig_completeness_error': metrics_ig.get('completeness_error_abs'),\n",
        "            'ig_mean_attribution': metrics_ig.get('mean_attribution'),\n",
        "            'ig_max_time': metrics_ig.get('max_attribution_time'),\n",
        "\n",
        "\n",
        "            # Rollout metrics\n",
        "            'rollout_entropy': metrics_rollout.get('entropy'),\n",
        "            'rollout_max_time': metrics_rollout.get('max_attention_time'),\n",
        "            'rollout_concentration': metrics_rollout.get('attention_concentration'),\n",
        "\n",
        "\n",
        "            # Grad×Attn metrics\n",
        "        }\n",
        "\n",
        "\n",
        "        all_results.append(result)\n",
        "\n",
        "        # CLEAR completo fine campione\n",
        "        torch.cuda.empty_cache()\n",
        "        gc.collect()\n",
        "\n",
        "        print(f\"\\n✓ Campione {idx}/{len(selected_samples)} completato\")\n",
        "\n",
        "        # Pausa stabilizzazione memoria (2 secondi)\n",
        "        import time\n",
        "        time.sleep(2)\n",
        "\n",
        "\n",
        "    # ============================================================\n",
        "    # SALVA RISULTATI AGGREGATI\n",
        "    # ============================================================\n",
        "    print(\"\\n\" + \"=\"*60)\n",
        "    print(\"SALVATAGGIO RISULTATI\")\n",
        "    print(\"=\"*60)\n",
        "\n",
        "\n",
        "    results_df = pd.DataFrame(all_results)\n",
        "    results_csv_path = os.path.join(OUTPUT_DIR, 'xai_metrics_summary.csv')\n",
        "    results_df.to_csv(results_csv_path, index=False)\n",
        "    print(f\"\\n✓ Metriche salvate: {results_csv_path}\")\n",
        "\n",
        "\n",
        "    # ============================================================\n",
        "    # VALIDAZIONE CLINICA: Confronto Controlli vs Pazienti\n",
        "    # ============================================================\n",
        "    print(\"\\n\" + \"=\"*60)\n",
        "    print(\"VALIDAZIONE CLINICA: Analisi Comparativa\")\n",
        "    print(\"=\"*60)\n",
        "\n",
        "\n",
        "    # Filtra solo predizioni corrette per analisi\n",
        "    correct_samples = results_df[results_df['correct'] == True]\n",
        "\n",
        "\n",
        "    if len(correct_samples) > 0:\n",
        "        controls = correct_samples[correct_samples['true_label'] == 'Controllo']\n",
        "        patients = correct_samples[correct_samples['true_label'] == 'Paziente']\n",
        "\n",
        "\n",
        "        print(f\"\\n📊 CONFRONTO CONTROLLI vs PAZIENTI (solo corretti)\")\n",
        "        print(\"-\"*60)\n",
        "\n",
        "\n",
        "        if len(controls) > 0 and len(patients) > 0:\n",
        "            print(f\"\\nControlli (n={len(controls)}):\")\n",
        "            print(f\"  Rollout entropy:        {controls['rollout_entropy'].mean():.3f} ± {controls['rollout_entropy'].std():.3f}\")\n",
        "            print(f\"  Attention concentration: {controls['rollout_concentration'].mean():.3f} ± {controls['rollout_concentration'].std():.3f}\")\n",
        "\n",
        "\n",
        "            print(f\"\\nPazienti (n={len(patients)}):\")\n",
        "            print(f\"  Rollout entropy:        {patients['rollout_entropy'].mean():.3f} ± {patients['rollout_entropy'].std():.3f}\")\n",
        "            print(f\"  Attention concentration: {patients['rollout_concentration'].mean():.3f} ± {patients['rollout_concentration'].std():.3f}\")\n",
        "\n",
        "\n",
        "            # Test statistico (se abbastanza campioni)\n",
        "            if len(controls) >= 5 and len(patients) >= 5:\n",
        "                from scipy.stats import mannwhitneyu\n",
        "\n",
        "\n",
        "                try:\n",
        "                    stat, p_value = mannwhitneyu(\n",
        "                        controls['rollout_entropy'].dropna(),\n",
        "                        patients['rollout_entropy'].dropna(),\n",
        "                        alternative='two-sided'\n",
        "                    )\n",
        "                    print(f\"\\nMann-Whitney U test (entropy):\")\n",
        "                    print(f\"  p-value = {p_value:.4f} {'✓ significativo (p<0.05)' if p_value < 0.05 else '(non significativo)'}\")\n",
        "                except:\n",
        "                    print(\"\\nTest statistico non eseguibile (campioni insufficienti)\")\n",
        "\n",
        "\n",
        "    # ============================================================\n",
        "    # ANALISI FAILURE MODE (se presente errore)\n",
        "    # ============================================================\n",
        "    errors = results_df[results_df['correct'] == False]\n",
        "\n",
        "\n",
        "    if len(errors) > 0:\n",
        "        print(\"\\n\" + \"=\"*60)\n",
        "        print(\"FAILURE MODE ANALYSIS\")\n",
        "        print(\"=\"*60)\n",
        "\n",
        "\n",
        "        for idx, row in errors.iterrows():\n",
        "            print(f\"\\n Errore: {row['filename']}\")\n",
        "            print(f\"   True: {row['true_label']}, Pred: {row['pred_label']}, Conf: {row['confidence']:.3f}\")\n",
        "            print(f\"\\n   Osservazioni XAI:\")\n",
        "            print(f\"   - Entropy: {row['rollout_entropy']:.3f}\")\n",
        "            print(f\"   - Concentration: {row['rollout_concentration']:.3f}\")\n",
        "            print(f\"\\n   → Controllare visualizzazioni per identificare cause dell'errore\")\n",
        "\n",
        "\n",
        "    # ============================================================\n",
        "    # RIEPILOGO FINALE\n",
        "    # ============================================================\n",
        "    print(\"\\n\" + \"=\"*60)\n",
        "    print(\"✓ ANALISI XAI COMPLETATA\")\n",
        "    print(\"=\"*60)\n",
        "\n",
        "\n",
        "    print(f\"\\n📁 STRUTTURA OUTPUT:\")\n",
        "    print(f\"   {OUTPUT_DIR}/\")\n",
        "    print(f\"   ├── integrated_gradients/    ({len(selected_samples)} visualizzazioni)\")\n",
        "    print(f\"   ├── attention_rollout/       ({len(selected_samples)} visualizzazioni)\")\n",
        "    print(f\"   ├── xai_metrics_summary.csv\")\n",
        "    print(f\"   └── selected_samples.csv\")\n",
        "\n",
        "\n",
        "    print(f\"\\n TOTALE VISUALIZZAZIONI: {len(selected_samples) * 2}\")\n",
        "    print(f\"\\n✓ Tutti i file salvati in: {OUTPUT_DIR}\")\n",
        "\n",
        "\n",
        "    print(\"\\n\" + \"=\"*60)\n",
        "    print(\"PROSSIMO STEP: Verifica visualizzazioni e crea slide\")\n",
        "    print(\"=\"*60)\n",
        "else:\n",
        "    print(\"- Skipping XAI execution: RUN_XAI = False\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LYxD-FtjG9dH"
      },
      "outputs": [],
      "source": [
        "# ============================================================\n",
        "# CONSENSO INTER-METODO: Correlation Analysis\n",
        "# ============================================================\n",
        "# Verifica concordanza tra IG e Rollout\n",
        "# ============================================================\n",
        "\n",
        "if RUN_XAI:\n",
        "    import numpy as np\n",
        "    from scipy.stats import pearsonr, spearmanr\n",
        "    import matplotlib.pyplot as plt\n",
        "    import seaborn as sns\n",
        "\n",
        "    print(\"=\"*60)\n",
        "    print(\"CONSENSO INTER-METODO: Correlation Analysis\")\n",
        "    print(\"=\"*60)\n",
        "\n",
        "    # Carica metriche salvate\n",
        "    metrics_df = pd.read_csv(os.path.join(OUTPUT_DIR, 'xai_metrics_summary.csv'))\n",
        "\n",
        "    # Filtra solo campioni con entrambi i metodi riusciti\n",
        "    valid_samples = metrics_df[\n",
        "        metrics_df['ig_max_time'].notna() &\n",
        "        metrics_df['rollout_entropy'].notna()\n",
        "    ]\n",
        "\n",
        "    print(f\"\\nCampioni con entrambi i metodi: {len(valid_samples)}/{len(metrics_df)}\")\n",
        "\n",
        "    if len(valid_samples) >= 3:\n",
        "        # ============================================================\n",
        "        # METRICA 1: Concordanza sui Peak Times\n",
        "        # ============================================================\n",
        "        print(\"\\n\" + \"-\"*60)\n",
        "        print(\"CONCORDANZA TEMPORALE (Peak Times)\")\n",
        "        print(\"-\"*60)\n",
        "\n",
        "        # Peak times (normalizzate per durata)\n",
        "        peak_times = {\n",
        "            'IG': valid_samples['ig_max_time'].values,\n",
        "            'Rollout': valid_samples['rollout_max_time'].values,\n",
        "        }\n",
        "\n",
        "        # Correlation matrix\n",
        "        methods = ['IG', 'Rollout']\n",
        "        corr_matrix = np.zeros((2, 2))\n",
        "\n",
        "        for i, m1 in enumerate(methods):\n",
        "            for j, m2 in enumerate(methods):\n",
        "                if i <= j:\n",
        "                    if i == j:\n",
        "                        corr_matrix[i,j] = 1.0\n",
        "                    else:\n",
        "                        corr, pval = pearsonr(peak_times[m1], peak_times[m2])\n",
        "                        corr_matrix[i,j] = corr\n",
        "                        corr_matrix[j,i] = corr\n",
        "\n",
        "                        sig = \"***\" if pval < 0.001 else \"**\" if pval < 0.01 else \"*\" if pval < 0.05 else \"ns\"\n",
        "                        print(f\"{m1} vs {m2}: r = {corr:.3f} (p = {pval:.4f}) {sig}\")\n",
        "\n",
        "        # Stampa correlazione\n",
        "        corr_ig_rollout = corr_matrix[0, 1]\n",
        "        print(f\"\\nCorrelazione IG vs Rollout: r = {corr_ig_rollout:.3f}\")\n",
        "\n",
        "        # ============================================================\n",
        "        # METRICA 2: Consenso su campioni corretti vs errori\n",
        "        # ============================================================\n",
        "        print(\"\\n\" + \"-\"*60)\n",
        "        print(\"PATTERN SU CORRETTI vs ERRORI\")\n",
        "        print(\"-\"*60)\n",
        "\n",
        "        correct_samples = valid_samples[valid_samples['correct'] == True]\n",
        "        error_samples = valid_samples[valid_samples['correct'] == False]\n",
        "\n",
        "        if len(correct_samples) > 0:\n",
        "            print(f\"\\nCorretti (n={len(correct_samples)}):\")\n",
        "            print(f\"  IG max time:         {correct_samples['ig_max_time'].mean():.2f}s ± {correct_samples['ig_max_time'].std():.2f}s\")\n",
        "            print(f\"  Rollout entropy:     {correct_samples['rollout_entropy'].mean():.3f} ± {correct_samples['rollout_entropy'].std():.3f}\")\n",
        "\n",
        "        if len(error_samples) > 0:\n",
        "            print(f\"\\nErrori (n={len(error_samples)}):\")\n",
        "            print(f\"  IG max time:         {error_samples['ig_max_time'].mean():.2f}s ± {error_samples['ig_max_time'].std():.2f}s\")\n",
        "            print(f\"  Rollout entropy:     {error_samples['rollout_entropy'].mean():.3f} ± {error_samples['rollout_entropy'].std():.3f}\")\n",
        "\n",
        "        # ============================================================\n",
        "        # METRICA 3: Consenso Controlli vs Pazienti\n",
        "        # ============================================================\n",
        "        print(\"\\n\" + \"-\"*60)\n",
        "        print(\"PATTERN CONTROLLI vs PAZIENTI (solo corretti)\")\n",
        "        print(\"-\"*60)\n",
        "\n",
        "        controls = correct_samples[correct_samples['true_label'] == 'Controllo']\n",
        "        patients = correct_samples[correct_samples['true_label'] == 'Paziente']\n",
        "\n",
        "        if len(controls) > 0 and len(patients) > 0:\n",
        "            print(f\"\\nControlli (n={len(controls)}):\")\n",
        "            print(f\"  Rollout entropy:    {controls['rollout_entropy'].mean():.3f}\")\n",
        "            print(f\"  Rollout peak time:  {controls['rollout_max_time'].mean():.2f}s\")\n",
        "\n",
        "            print(f\"\\nPazienti (n={len(patients)}):\")\n",
        "            print(f\"  Rollout entropy:    {patients['rollout_entropy'].mean():.3f}\")\n",
        "            print(f\"  Rollout peak time:  {patients['rollout_max_time'].mean():.2f}s\")\n",
        "\n",
        "            # Differenza percentuale\n",
        "            entropy_diff = abs(patients['rollout_entropy'].mean() - controls['rollout_entropy'].mean())\n",
        "            time_diff = abs(patients['rollout_max_time'].mean() - controls['rollout_max_time'].mean())\n",
        "\n",
        "            print(f\"\\nDifferenze:\")\n",
        "            print(f\"  Entropy: {entropy_diff:.3f} ({entropy_diff/controls['rollout_entropy'].mean()*100:.1f}%)\")\n",
        "            print(f\"  Peak time: {time_diff:.2f}s\")\n",
        "\n",
        "        # ============================================================\n",
        "        # INTERPRETAZIONE\n",
        "        # ============================================================\n",
        "        print(\"\\n\" + \"=\"*60)\n",
        "        print(\"INTERPRETAZIONE\")\n",
        "        print(\"=\"*60)\n",
        "\n",
        "        # Correlazione (ora è un singolo valore, non media)\n",
        "        avg_corr = corr_matrix[0,1]\n",
        "\n",
        "        print(f\"\\nCorrelazione inter-metodo: {avg_corr:.3f}\")\n",
        "\n",
        "        if avg_corr > 0.7:\n",
        "            print(\"✓ ALTA concordanza tra metodi (r > 0.7)\")\n",
        "            print(\"  → I 2 metodi identificano regioni temporali simili\")\n",
        "            print(\"  → Maggior affidabilità delle attribution\")\n",
        "        elif avg_corr > 0.4:\n",
        "            print(\"◐ MEDIA concordanza tra metodi (0.4 < r < 0.7)\")\n",
        "            print(\"  → I metodi catturano aspetti parzialmente sovrapposti\")\n",
        "            print(\"  → Interpretare con cautela le differenze\")\n",
        "        else:\n",
        "            print(\"    BASSA concordanza tra metodi (r < 0.4)\")\n",
        "            print(\"  → I metodi catturano aspetti diversi\")\n",
        "\n",
        "        print(\"\\n\" + \"=\"*60)\n",
        "\n",
        "    else:\n",
        "        print(\"\\n    Troppi pochi campioni validi per correlation analysis\")\n",
        "        print(f\"   Serve almeno 3 campioni, trovati: {len(valid_samples)}\")\n",
        "else:\n",
        "    print(\"- Skipping XAI consensus analysis: RUN_XAI = False\")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "wsS8lFqrmuks",
        "s7c3rxjImyxe",
        "SGXZvV-Vm2CO",
        "KUC1Aa5DnH8l",
        "S4NoPX18nkNo"
      ],
      "gpuType": "L4",
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}